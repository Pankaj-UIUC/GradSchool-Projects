---
title: "Stat542_pankajs2_Project"
author: "Pankaj Sharma"
date: "4/23/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
set.seed(42)
```

```{r, warning=FALSE, message=FALSE, echo = FALSE}
library(caret)
library(tidyverse)
library(rsample)
library(kableExtra)
library(mice)
library(randomForest)
library(gbm)
library(NbClust)
library(ggplot2)
library(ggthemes)
library(viridis)
library(cluster)
library(maps)
library(usmap)
library(FactoMineR)
library(factoextra)
library(kohonen)
library(urbnmapr)
library(xgboost)
```

```{r load-data, echo = FALSE}
covid_data = read.csv("https://teazrq.github.io/stat542/data/county_data_apr22.csv")
```

# Data

The data is downloaded from [Github data](https://github.com/Yu-Group/covid19-severity-prediction) processed and constructed by Prof. Bin Yu's group at Berekely. It contains county level information about the cases and deaths due to Covid-19 and also includes demographical information about population groups and hospital information. However before doing any modelling on the data, we need to properly handle the missing data. To start with we have two sets of columns of `Latitude` and `Longitude`, since both are almost the same, we will only keep the ones showing population centres. For colums which have too many rows missing data like `MortalityAge1.4Years2015.17, MortalityAge5.14Years2015.17,mortality2015.17Estimated`, we simply delete the columns as there is not enough information to do any imputation.

For columns like `Diabetes Percentage, Stroke Mortality, and Heart Disease Mortality` it makes sense to replace the missing values with mean since these are continuous variables and there are not many missing values. For the colums related to `Stay at Home`, it is given the the values represent the day number on which the orders were issued. It is reasonable to assume that for the counties that are missing values in these columns, thre may not have been an offical Stay at Home order issued so there is no day count. Hence, one specific value is assigned to these columns to represent the missing Stay at Home order. For `Medicare Enrolled`, national mean percentage of covered was used to calculate the eligible using the population of the county. 

For other columns left with missing values, imputation was run using the `Mice` package in R and using `Random Forest` for imputation. `Random Forest` is chosen as it is one of the best methods to make sure the variables don't become too inter-correlated and hence will make them more useful for modelling later. 

```{r missing-values, warning = FALSE, message = FALSE, echo = FALSE}
# Pop_latitude and Pop_latitude almost identical to lat and long and dont have missing values
covid_data$lat = NULL
covid_data$lon = NULL

# For missing values in mortality column, replacing NA values with national average seems reasonable
covid_data$X3.YrMortalityAge.1Year2015.17[is.na(covid_data$X3.YrMortalityAge.1Year2015.17)] = mean(covid_data$X3.YrMortalityAge.1Year2015.17, na.rm = TRUE)

# For 7 counties missing eligible for care, national eligiblity mean % was applied
national_covered_mean = mean(covid_data$X.EligibleforMedicare2018 / covid_data$PopulationEstimate2018,
                             na.rm = TRUE)
covid_data$X.EligibleforMedicare2018[is.na(covid_data$X.EligibleforMedicare2018)] = national_covered_mean * covid_data$PopulationEstimate2018[is.na(covid_data$X.EligibleforMedicare2018)]

# For Medicare Enrollement Aged - all misisng values replaced with zero
covid_data$MedicareEnrollment.AgedTot2017[is.na(covid_data$MedicareEnrollment.AgedTot2017)] = 0

covid_data$DiabetesPercentage[is.na(covid_data$DiabetesPercentage)] = mean(covid_data$DiabetesPercentage, na.rm = TRUE)
covid_data$HeartDiseaseMortality[is.na(covid_data$HeartDiseaseMortality)] = mean(covid_data$HeartDiseaseMortality, na.rm = TRUE)
covid_data$StrokeMortality[is.na(covid_data$StrokeMortality)] = mean(covid_data$StrokeMortality, na.rm = TRUE)
covid_data$dem_to_rep_ratio[is.na(covid_data$dem_to_rep_ratio)] = mean(covid_data$dem_to_rep_ratio, na.rm = TRUE)

# Foreign travel ban column only has one value which we think represent the ban being implemented hence we can convert into a factor variable

covid_data$foreign.travel.ban = 1
covid_data$foreign.travel.ban = as.factor(covid_data$foreign.travel.ban)

# Assuming that missing values in stay-at-home column represent the stay-at-home order was not issued at all
covid_data$stay.at.home[is.na(covid_data$stay.at.home)] = 999999
covid_data$X.50.gatherings[is.na(covid_data$X.50.gatherings)] = 999999
covid_data$X.500.gatherings[is.na(covid_data$X.500.gatherings)] = 999999
covid_data$entertainment.gym[is.na(covid_data$entertainment.gym)] = 999999
# Too many missing values for columns, no reasonable way to get any info from them
covid_data$X3.YrMortalityAge1.4Years2015.17 = NULL
covid_data$X3.YrMortalityAge5.14Years2015.17 = NULL
covid_data$mortality2015.17Estimated = NULL

covid_data$SVIPercentile[is.na(covid_data$SVIPercentile)] = median(covid_data$SVIPercentile, na.rm = TRUE)
covid_data$HPSAShortage[is.na(covid_data$HPSAShortage)] = mean(covid_data$HPSAShortage, na.rm = TRUE)

# All counties with 3 Yr diabetes missing are very small in population so assuming 0 for those
covid_data$X3.YrDiabetes2015.17[is.na(covid_data$X3.YrDiabetes2015.17)] = 0

# Mortality rates age wise ar eimputed using random forest method in the mice package for imputation
set.seed(42)
imputed = mice(
  covid_data[, 64:71],
  method = "rf",
  m = 3,
  maxit = 3,
  printFlag = FALSE
)
completed_data = complete(imputed, 1)
covid_data[, 64:71] = completed_data
imputed = mice(
  covid_data[, 13:83],
  method = "rf",
  m = 3,
  maxit = 3,
  printFlag = FALSE
)
completed_data = complete(imputed, 1)
covid_data[, 13:83] = completed_data

missing_values = as.data.frame(sapply(covid_data, function(x)
  sum(is.na(x))))
df <- cbind(var_name = rownames(missing_values), missing_values)
rownames(df) <- 1:nrow(df)
df = df %>%
  rename(missing_count = "sapply(covid_data, function(x) sum(is.na(x)))")
#df[df$missing_count > 0, ]

rm(completed_data, imputed, missing_values, df)

```

# Unsupervised Learning

## Clustering

Clustering is a technique which groups observations which have similar charecteristics in a dataset. The observations which belong to a particular group are closer in nature to each other than those from others groups. This helps in the exploraatory phase of the analysing the data and fingidng hidden patterns.  

Our aim is to cluster counties which show resembling patterns based on demographics and health indicators. We use three clustering methods to find answers from within the data. 

- K-Means Clustering
- Hierarchial Clustering with PCA

### K-Means Clustering

One of the most widely used method of clustering, K-Means is an iterative method to assign observations to clusters. K-Means algorithm groups counties into different clusters based on population density, population demographic, health status of county and health care quality.

We perform K- Means clustering on select variables such as 

 - `PopulationDensityperSqMile2010`
 - `CensusPopulation2010`
 - `MedianAge2010`
 - `MedicareEnrollment.AgedTot2017`
 - `StrokeMortality`
 - `X.Hospitals`
 - `HPSAShortage`
 - `HeartDiseaseMortality`
 - `Smokers_Percentage`
 - `RespMortalityRate2014`
 - `X.ICU_beds`
 - `SVIPercentile`
 - `dem_to_rep_ratio`
 
Since, K-Means require an initial number of cluster, we use the Elbow-Method to determine optimal number of clusters. This approach tests a range of number of clusters calculating the sum of squared errors against each value. We have chosen the K value 5, at which the increase in clusters causes a small decrease in error. K-Means clustering for our data creates 5 clusters with the highest number of counties lying in cluster 3. 

```{r k-means, echo = FALSE, fig.align = "center", fig.height = 6, fig.width = 12}

# Creating a subset of variables which are needed
clust_data = covid_data[, c(18, 19, 20, 22, 25, 26, 27, 28, 32, 33, 34, 80, 81)]

# to get the optimal number of clusters we use elbow method
clust_data_mat = as.matrix(clust_data)

## Elbow method
wss = (nrow(clust_data_mat) - 1) * sum(apply(clust_data_mat, 2, var))
  for (i in 2:30) wss[i] = sum(kmeans(clust_data_mat,
                                       centers = i, iter.max = 30)$withinss)

p01 = ggplot(as.data.frame(cbind(wss, seq(1,30))), aes(x = V2, y = wss)) +
  geom_line(linetype = "dashed") +
  geom_point() + 
  ggtitle("Fig 1 - Within groups Sum of Square vs Clusters") +
  xlab("Number of Clusters") + ylab("Within groups sum of squares")

set.seed(1)
kmeans_cluster = kmeans(clust_data_mat, centers = 5, iter.max = 30)

#Binding clusters and base dataset
covid_data_test = cbind(covid_data, kmeans_cluster$cluster)
names(covid_data_test)[names(covid_data_test) == "kmeans_cluster$cluster"] = "cluster"

kmeans_cluster_table = tibble(
  "Cluster" = c("1", "2", "3", "4", "5") ,
  "Number of Counties" = c(table(covid_data_test$cluster)[1],
                           table(covid_data_test$cluster)[2],
                           table(covid_data_test$cluster)[3],
                           table(covid_data_test$cluster)[4],
                           table(covid_data_test$cluster)[5])
)

kable(kmeans_cluster_table, digits = 5,
      caption = "Table: K-Means Clusters") %>%
  kable_styling("striped", full_width = FALSE) %>%
  column_spec(column = 1, bold = TRUE)


## Plot
df = counties
df$county_fips = as.integer(df$county_fips)

county_map = left_join(covid_data_test, df, by = c("countyFIPS" = "county_fips"))

p02 = county_map %>%
  ggplot(aes(long, lat, group = group, fill = cluster)) +
  geom_polygon(aes(fill = cluster), color = "black") +
  scale_fill_viridis_c(option = "C") +
  ggtitle("Fig 2 - K-Means clustering of US Counties") +
  xlab("longitude") + ylab("latitude")

```


We now analyse the clusters and their underlying pattern. 

We first analyse the ratio of death to populations across counties to see which clusters have the maximum deaths with respect to the populations. It is observed that clusters 2 and 5 have the highest death to population ratio, and we further deep dive into details of these clusters. 

- The maximum number of deaths have been in Cluster 5 and the number of confirmed cases being 4 in every 10000 people
- The most number of counties in Clusters 2 and 5 belong to the states California, New York and Texas
- Cluster 3 has maximum number of counties since these are the ones across US which are not as badly hit by the virus as the those in Cluster 2 and 5
- The cluster with highest number of deaths are performing worst in terms of ICU beds to total confirmed cases, this is a very important factor highlighting that the healthcare facilities were not upto mark in these counties which resulted in more deaths. 

```{r k-means-analysis, echo = FALSE, fig.align = "center", fig.height = 6, fig.width = 12}
cluster_level_data = covid_data_test %>%
  group_by(cluster) %>%
  summarise(number = n(),
            population = sum(CensusPopulation2010),
            deaths = sum(tot_deaths),
            confirmed_casses = sum(tot_cases),
            icu_beds = sum(X.ICU_beds),
            stroke_mort = mean(StrokeMortality),
            mean_median_age = mean(MedianAge2010),
            sum_hospitals = mean(X.Hospitals),
            avg_svi = mean(SVIPercentile),
            avg_diab = mean(DiabetesPercentage))

# Deaths/Population
cluster_level_data$death_ratio = (cluster_level_data$deaths/cluster_level_data$population)*100

p03 = ggplot(data = cluster_level_data, aes(x = cluster, y = death_ratio)) +
  geom_bar(stat = "identity", fill = "deepskyblue") + 
  ggtitle("Fig 3 - Death to population ratio across clusters") +
  xlab("Clusters") + ylab("Death/Population")


# Cases/Population
cluster_level_data$case_ratio = (cluster_level_data$confirmed_casses/cluster_level_data$population)*100

p04 = ggplot(data = cluster_level_data, aes(x = cluster, y = case_ratio)) +
  geom_bar(stat = "identity", fill = "deepskyblue") + 
  ggtitle("Fig 4 - Cases to population ratio across clusters") +
  xlab("Clusters") + ylab("Cases/Population")

#States/Regions
kmeans_subset_data = subset(covid_data_test, cluster == 2 | cluster == 5)

kmeans_subset_data2 = kmeans_subset_data %>%
  group_by(State) %>%
  summarise(number = n())
kmeans_subset_data2$State = as.character(kmeans_subset_data2$State)
kmeans_subset_data2[1, 1] = as.character("Hawaii")
kmeans_subset_data2 = kmeans_subset_data2[order(-kmeans_subset_data2$number),]

state_region_split = tibble(
  "State" = kmeans_subset_data2$State ,
  "Number of Counties" = c(kmeans_subset_data2$number)
  )
kable(state_region_split, digits = 5,
      caption = "Number of Counties across States - Cluster 2 and 5") %>%
  kable_styling("striped", full_width = FALSE) %>%
  column_spec(column = 1, bold = TRUE)

#ICU beds/Confirmed Cases
cluster_level_data$icu_beds_pop = (cluster_level_data$icu_beds/cluster_level_data$confirmed_casses)*100
p05 = ggplot(data = cluster_level_data, aes(x = cluster, y = icu_beds_pop)) +
  geom_bar(stat = "identity", fill = "deepskyblue") + 
  ggtitle("Fig 5 - ICU Beds to Cases ratio across clusters") +
  xlab("Clusters") + ylab("ICU Beds/Confirmed Cases")
```



#### Hierarchial Clustering and PCA

In order to get better clustering results, we use the help of PCA method to extract variables which are important and explain the variation in the data. We begin with prepping the data by subsetting variables based on judgement and requirement of PCA, i.e numerical and non-zero variables.

We narrowed down 24 variables contributing the most to 1st and 2nd PCA dimensions and use those for hierachial clustering.

Hierarchial clustering  has two types, agglomerative and divisive. We perform Divisive Hierarchial clustering, using the function `diana` which is a top-down approach. In this approach all the observations are assigned a single cluster and then are further partitioned into similar clusters until each observation has a cluster.

We get 8 clusters based on hierarchial clustering and we further dig deeper into these clusters. 

```{r hc, echo = FALSE, fig.align = "center", fig.height = 6, fig.width = 12, message = FALSE, warning = FALSE}

#HClust and PCA
## Doing data prep, excluding factor variables and 0 variance variables
pca_data = covid_data[,-c(as.integer(which(sapply(
  covid_data, is.factor
))))]
pca_data = pca_data[, which(apply(pca_data, 2, var) != 0)]
pca_data = pca_data[,-c(77:245)]

#PCA
pca_results = PCA(pca_data, scale.unit = TRUE, graph = FALSE)
fviz_screeplot(pca_results, addlabels = TRUE, ylim = c(0, 80))

#Choosing Variables for Hierachial Clustering
#Variables
vars = get_pca_var(pca_results)
M = as.matrix(as.matrix(sort(vars$contrib[, 1], decreasing = TRUE))[c(1:20), ])
M2 = as.matrix(as.matrix(sort(vars$contrib[, 2], decreasing = TRUE))[c(1, 2, 5, 6),])
variables_hclust = (c(row.names(M), row.names(M2)))

#HClust
hclust_data = covid_data[, c(variables_hclust)]
hclust_data = scale(hclust_data)

#Divisive Hierarchial Clustering

hclusters = diana(hclust_data)
clusters = cutree(hclusters, k = 8)

#Merging with main data set
hclust_covid_data = as.data.frame(covid_data %>% mutate(cluster = clusters))
hclusters_table = tibble(
  "Cluster" = c("1", "2", "3", "4", "5", "6", "7", "8") ,
  "Number of Counties" = c(
    table(hclust_covid_data$cluster)[1],
    table(hclust_covid_data$cluster)[2],
    table(hclust_covid_data$cluster)[3],
    table(hclust_covid_data$cluster)[4],
    table(hclust_covid_data$cluster)[5],
    table(hclust_covid_data$cluster)[6],
    table(hclust_covid_data$cluster)[7],
    table(hclust_covid_data$cluster)[8]
  )
)

kable(hclusters_table, digits = 5,
      caption = "Table: Hierarchial Clusters") %>%
  kable_styling("striped", full_width = FALSE) %>%
  column_spec(column = 1, bold = TRUE)

#
df = counties
df$county_fips = as.integer(df$county_fips)

county_map = left_join(hclust_covid_data, df, by = c("countyFIPS" = "county_fips"))

p06 = county_map %>%
  ggplot(aes(long, lat, group = group, fill = cluster)) +
  geom_polygon(aes(fill = cluster), color = "black") +
  scale_fill_viridis_c(option = "C") +
  ggtitle("Fig 6 - Hierachial clustering of US Counties") +
  xlab("longitude") + ylab("latitude")

```

# Supervised Learning

For supervised learning, we focus on both classification and regression tasks. 

## Classification

For classification, we first create the response variable using `tot_deaths/PopulationEstimate2018`. We split the processed data after doing the imputation into train and test datasets. We will use the training dataset to train our models and use the testing dataset to make predictions and check for accuracy. To tune the hyperparameters for the models, we wil use `cross-validation`, for the purposes of modelling, we are using the inbuilt functions in the `caret` library. Some of the colums which are just serial numbers or columns which don't contain discriminating information are removed before training the model on the data. 

We have considered the following models for the purposes of the classification:

- Random Forest: We used the inbuilt `train` function in the `caret` library with method set to `rf`.

  - Hyperparameter `Mtry` was tuned using 5-fold cross validation with a tunelength of 15, 
    The final value used for the model was mtry = 54.

- Gradient Boosting Method: We used the inbuild `train` function in the `caret` library with method set to `gbm`

  - Hyprparameters `ntrees, shrinkage, interaction.depth` and `n.minobsinnode` were tuned using 5-fold cross-validation.

```{r test-train-class, echo = FALSE, warning = FALSE, message = FALSE}
# making a copy of the data for classification
class_data = covid_data

# Removing daily cases and deaths since we have the total cases and deaths variable
class_data[, 84:269] = NULL

# creating the response variable
class_data$dpt = as.factor(ifelse((class_data$tot_deaths / class_data$PopulationEstimate2018) *
                                    100000 > 1,
                                  "pos",
                                  "neg"
))

#table(class_data$dpt)

# Removing variables which like serial numbers etc and total deaths
class_data$X = NULL
class_data$tot_deaths = NULL
class_data$StateName = NULL
class_data$countyFIPS = NULL
class_data$CountyName = NULL
class_data$Rural.UrbanContinuumCode2013 = as.factor(class_data$Rural.UrbanContinuumCode2013)

# Foregin travel ban only has one level
class_data$foreign.travel.ban = NULL

# Federal guideline has no variation
class_data$federal.guidelines = NULL
class_data$State = NULL

set.seed(42)
split = initial_split(class_data, prop = 0.80)
train = training(split)
test = testing(split)
```


### Random Forest

```{r, cv-control-rf, echo = TRUE}
cv_control = trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)
```

```{r, rf, echo = FALSE, message = FALSE, cache = TRUE}
set.seed(42)
mod_rf = train(
  form = dpt ~ .,
  data = train,
  method = "rf",
  trControl = cv_control,
  metric = 'ROC',
  tuneLength = 15,
  verbose = FALSE
)
mod_rf

```


```{r, cv-control-gbm, echo = TRUE}
cv_control = trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

grid_gbm <-
  expand.grid(
    n.trees = c(100, 500, 1000),
    shrinkage = c(0.01, 0.05, 0.1, 0.5),
    n.minobsinnode = c(3, 5, 10),
    interaction.depth = c(1, 5, 10)
  )
```

```{r, gbm, echo = TRUE, message = FALSE, cache = TRUE}
set.seed(42)
mod_gbm = train(
  form = dpt ~ .,
  data = train,
  method = "gbm",
  trControl = cv_control,
  metric = 'ROC',
  tuneLength = 10,
  tuneGrid = grid_gbm,
  verbose = FALSE
)

```

The final values used for the model were n.trees = 100, interaction.depth = 5, shrinkage = 0.01 and n.minobsinnode = 5.


Based on the ROC, best model out of the two tuned, Random Forest and Gradient Boosting, the Gradient Boosting model with the tuned hyperparameters was selected to make predictions on the test data. Performance metrics of the model on the test data are shown below.

```{r results-class, echo = FALSE}
result = tibble(
  "Model Name" = c("Random Forest",
                   "Gradient Boosting Method") ,
  "ROC" = c(max(mod_rf$results[2]),
            max(mod_gbm$results[5])),
  "Sensitivity" = c(max(mod_rf$results[3]),
                    max(mod_gbm$results[6]))
)

kable(result, digits = 5,
      caption = "Cross Validated ROC for Random Forest and Gradient Boosting") %>%
  kable_styling(
    c("striped", "bordered", "hover"),
    full_width = FALSE,
    latex_options = c("striped", "hold_position")
  ) %>%
  column_spec(column = 1, bold = TRUE)
```

```{r test-pred, echo = TRUE}
confusionMatrix(predict(mod_gbm, test, type = "raw"),
                reference = test$dpt,
                positive = "pos")
```



Using the selected model on the testing data, we are able to achive an accuracy of nearly 83% which is pretty good in this context. Using these models, we also checked to see which variables are most effective in making accurate classifications. In the tables below, we see the top-10 variables ordered by their importance in the classification for both the Gradient Boosting Method and Random Forest Method. Total cases as expected is the most important variable in both the models as it is directly realted to the total number of deaths. `Heart Disease Mortality` and `Daibetes Percentage` seem to important too indicating that more vulnerable population is effected more and is very discriminative in making the predictons. 
In the GBM model, `Stay at Home` and healthcare factors are very important in making the accurate classifications indicating that the places where the healthcare system wasn't overwhelmed had less deaths as compared to other counties. Another important factor is `Population Density` which relate to the $R_{0}$ factor of the infection. This information in conjunction with the domain knowledge of the virus can be used to take actions in certain areas and reduce the death count.


#### Variable Importance

```{r varimp-gbm, echo = FALSE}
gbmimp <- varImp(mod_gbm, scale = FALSE)
gbmimp = as.data.frame(gbmimp$importance)
gbmimp <- cbind(var_name = rownames(gbmimp), gbmimp)
rownames(gbmimp) <- 1:nrow(gbmimp)
gbmimp = gbmimp[gbmimp$Overall > 0,]
gbmimp = gbmimp[order(gbmimp$Overall, decreasing = TRUE), ]

rf_imp <- varImp(mod_rf, scale = FALSE)
rf_imp = as.data.frame(rf_imp$importance)
rf_imp <- cbind(var_name = rownames(rf_imp), rf_imp)
rownames(rf_imp) <- 1:nrow(rf_imp)
rf_imp = rf_imp[rf_imp$Overall > 0,]
rf_imp = rf_imp[order(rf_imp$Overall, decreasing = TRUE), ]
```


```{r varimp-table, echo = FALSE, warning = FALSE, message = FALSE}
kable(gbmimp[1:10, ],
      digits = 5,
      caption = "Variable Importance for prediction(GBM Model)",
      align = "clc") %>%
  kable_styling(
    c("striped", "bordered", "hover"),
    full_width = FALSE,
    latex_options = c("striped", "hold_position")
  ) %>%
  column_spec(column = 1, bold = TRUE)

kable(rf_imp[1:10, ],
      digits = 5,
      caption = "Variable Importance for prediction(RF Model)",
      align = "clc") %>%
  kable_styling(
    c("striped", "bordered", "hover"),
    full_width = FALSE,
    latex_options = c("striped", "hold_position")
  ) %>%
  column_spec(column = 1, bold = TRUE)
```


\newpage

## Regression

To predict the deaths one week ahead, we need to capture the change occuring in the deaths and cases as we move across time. Instead of taking a time series approach, we approached this using feature engineering. Also, Instead of predicting total deaths after one week into the future, we will use our models to predict the deaths occured in the week after the data ends and then will add the given deaths till date to the predicted values to get the total deaths by the end of the week. To achieve this, we first created variables calculating the change in death and cases every 7 days as we move back from the last day data contained in the dataset. This allowed us to attain weekly change in cases and deaths and accounted for the change in cases/deaths or we can call it slope as well. Again we removed the variables like Serial Numbers and other variables which had no variation in them throughout the dataset. Once we had the dataset ready, we split it into training and testing datasets. We will train the following three models on the training data:

- Random Forest: We used the inbuilt `train` function in the `caret` library with method set to `rf`.

  - Hyperparameter `Mtry` was tuned using 5-fold cross validation with 3 repeats with a tunelength of 10
  
- Penalized Linear Regression: `alpha` which decides if the penalty applied is l1 or l2 and `lambda` the regularization parameter were tuned using 
5-fol cross validation in the `train` function in the `caret` library using `glmnet` method.

- Xtreme Gradient Boosting: `eta` , `max_depth` and `colsample_bytree` were tuned using 5-fold cross-validation, rest of the hyperparameters were kept constant.

Best tune parameters for all the three methods is shown below. Along with that, we have also shown the minimum Root Mean Square Error achieved with each mode. We finally decided to use `XG Boost` for making predictions on the testing data and calculating the Test RMSE.

```{r reg-data, echo = FALSE, warning = FALSE, message = FALSE}
reg_data = covid_data
target = reg_data$tot_deaths
reg_data$deathscurrentweek = reg_data[, "X.Deaths_04.22.2020"] - reg_data[, "X.Deaths_04.15.2020"]
reg_data$deaths = NULL
reg_data$cases = NULL
reg_data = select(reg_data,-c(261:269))
reg_data$last7daysdeaths = reg_data[, "X.Deaths_04.15.2020"] - reg_data[, "X.Deaths_04.08.2020"]
reg_data$prev7daysdeaths = reg_data[, "X.Deaths_04.08.2020"] - reg_data[, "X.Deaths_04.01.2020"]
reg_data$marchweek5 = reg_data[, "X.Deaths_04.01.2020"] - reg_data[, "X.Deaths_03.25.2020"]
reg_data$marchweek4 = reg_data[, "X.Deaths_03.25.2020"] - reg_data[, "X.Deaths_03.18.2020"]
reg_data$marchweek3 = reg_data[, "X.Deaths_03.18.2020"] - reg_data[, "X.Deaths_03.11.2020"]
reg_data$marchweek2 = reg_data[, "X.Deaths_03.11.2020"] - reg_data[, "X.Deaths_03.04.2020"]
reg_data$marchweek1 = reg_data[, "X.Deaths_03.04.2020"] - reg_data[, "X.Deaths_02.26.2020"]
reg_data$febweek4 = reg_data[, "X.Deaths_02.26.2020"] - reg_data[, "X.Deaths_02.19.2020"]
reg_data$febweek3 = reg_data[, "X.Deaths_02.19.2020"] - reg_data[, "X.Deaths_02.12.2020"]
reg_data$febweek2 = reg_data[, "X.Deaths_02.12.2020"] - reg_data[, "X.Deaths_02.05.2020"]
reg_data$febweek1 = reg_data[, "X.Deaths_02.05.2020"] - reg_data[, "X.Deaths_01.29.2020"]
reg_data$janweek4 = reg_data[, "X.Deaths_01.29.2020"] - reg_data[, "X.Deaths_01.22.2020"]
reg_data$last7dayscases = reg_data[, "X.Cases_04.15.2020"] - reg_data[, "X.Cases_04.08.2020"]
reg_data$prev7dayscases = reg_data[, "X.Cases_04.08.2020"] - reg_data[, "X.Cases_04.01.2020"]
reg_data$marchCases_week5 = reg_data[, "X.Cases_04.01.2020"] - reg_data[, "X.Cases_03.25.2020"]
reg_data$marchCases_week4 = reg_data[, "X.Cases_03.25.2020"] - reg_data[, "X.Cases_03.18.2020"]
reg_data$marchCases_week3 = reg_data[, "X.Cases_03.18.2020"] - reg_data[, "X.Cases_03.11.2020"]
reg_data$marchCases_week2 = reg_data[, "X.Cases_03.11.2020"] - reg_data[, "X.Cases_03.04.2020"]
reg_data$marchCases_week1 = reg_data[, "X.Cases_03.04.2020"] - reg_data[, "X.Cases_02.26.2020"]
reg_data$febCases_week4 = reg_data[, "X.Cases_02.26.2020"] - reg_data[, "X.Cases_02.19.2020"]
reg_data$febCases_week3 = reg_data[, "X.Cases_02.19.2020"] - reg_data[, "X.Cases_02.12.2020"]
reg_data$febCases_week2 = reg_data[, "X.Cases_02.12.2020"] - reg_data[, "X.Cases_02.05.2020"]
reg_data$febCases_week1 = reg_data[, "X.Cases_02.05.2020"] - reg_data[, "X.Cases_01.29.2020"]
reg_data$janCases_week4 = reg_data[, "X.Cases_01.29.2020"] - reg_data[, "X.Cases_01.22.2020"]
reg_data = select(reg_data,-c(84:260))

# Removing variables which like serial numbers etc and total deaths
reg_data$X = NULL
reg_data$countyFIPS = NULL
reg_data$StateName = NULL
reg_data$CountyName = NULL
reg_data$Rural.UrbanContinuumCode2013 = as.factor(reg_data$Rural.UrbanContinuumCode2013)

# Foregin travel ban only has one level
reg_data$foreign.travel.ban = NULL

# Federal guideline has no variation
reg_data$federal.guidelines = NULL
reg_data$State = NULL
```

```{r test-train-split-reg, echo = FALSE, warning = FALSE, message = FALSE}
set.seed(42)
split_reg = initial_split(reg_data, prop = 0.80)
train_reg = training(split_reg)
test_reg = testing(split_reg)
```

```{r, cv-control-rf-reg, echo = FALSE}
cv_control = trainControl(method = "cv",
                          number = 5)
```

```{r, rf-reg, echo = FALSE, message = FALSE, cache = TRUE}
set.seed(42)
mod_rf_reg = train(
  form = deathscurrentweek ~ .,
  data = train_reg,
  method = "rf",
  trControl = cv_control,
  metric = 'RMSE',
  tuneLength = 10,
  verbose = FALSE
)
```


```{r penalized-reg, warning = FALSE, message = FALSE, echo = FALSE, cache = TRUE}
tuneGrid_lasso = expand.grid(.alpha = seq(0, 1, by = 0.5),
                             .lambda = seq(0, 100, by = 0.1))

trainControl <- trainControl(method = "cv",
                             number = 5)
set.seed(42)
mod_lasso = train(
  form = deathscurrentweek ~ .,
  data = train_reg,
  method = "glmnet",
  trControl = trainControl,
  metric = "RMSE",
  tuneGrid = tuneGrid_lasso,
  family = "gaussian"
)

```



```{r xgb-reg, echo = FALSE, cache = TRUE, message = FALSE, warning = FALSE}
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
  nrounds = 1000,
  eta = c(0.01, 0.001, 0.0001),
  max_depth = c(2, 4, 6, 8, 10),
  gamma = 1,
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = seq(0.5, 0.9, length.out = 5)
)

# pack the training control parameters
xgb_trcontrol_1 = trainControl(
  method = "cv",
  number = 5,
  verboseIter = FALSE,
  allowParallel = TRUE
)
set.seed(42)
xgb_train_1 = train(
  form = deathscurrentweek ~ .,
  data = train_reg,
  trControl = xgb_trcontrol_1,
  tuneGrid = xgb_grid_1,
  method = "xgbTree",
  metric = "RMSE"
)
```


```{r reg-results, echo = FALSE, message = FALSE, warning = FALSE}
result = tibble(
  "Model Name" = c("Random Forest",
                   "XG Boost",
                   "Penalized Linear Regression") ,
  "RMSE" = c(
    min(mod_rf_reg$results[2]),
    min(xgb_train_1$results[8]),
    min(mod_lasso$results[3])
  )
)

kable(result, digits = 5,
      caption = "Table: Cross Validated RMSE for Random Forest, XGBoost and Penalized LR") %>%
  kable_styling(
    "striped",
    full_width = FALSE,
    latex_options = c("striped", "hold_position")
  ) %>%
  column_spec(column = 1, bold = TRUE)

```

```{r test-data, warning = FALSE, message = FALSE, echo = FALSE}
set.seed(42)
pred_test = predict(xgb_train_1, test_reg)
test_rmse = sqrt(mean((test_reg$deathscurrentweek - pred_test) ^ 2))
test_rmse
```


```{r xgb-reg-full, echo = FALSE, cache = TRUE, message = FALSE, warning = FALSE}
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
  nrounds = xgb_train_1$bestTune$nrounds,
  eta = xgb_train_1$bestTune$eta,
  max_depth = xgb_train_1$bestTune$max_depth,
  gamma = xgb_train_1$bestTune$gamma,
  min_child_weight = xgb_train_1$bestTune$min_child_weight,
  subsample = xgb_train_1$bestTune$subsample,
  colsample_bytree = xgb_train_1$bestTune$colsample_bytree
)

# pack the training control parameters
xgb_trcontrol_1 = trainControl(
  method = "cv",
  number = 5,
  verboseIter = FALSE,
  allowParallel = TRUE
)
set.seed(42)
xgb_train_full = train(
  form = deathscurrentweek ~ .,
  data = train_reg,
  trControl = xgb_trcontrol_1,
  tuneGrid = xgb_grid_1,
  method = "xgbTree",
  objective = "reg:squarederror",
  metric = "RMSE"
)
```

### Making predictions for April 29

Once we have our models trained and tested, we decided to make predictions on the completely unseen and new data. This data is downloaded using python script present on the github repo mentioned above where we got the original data from. We will only keep the data till Apr 29 in this dataset and delete the columns containing data of the days after that. 

We will replicate the missing value imputation process that we applied on the original data to get rid of missing data in this dataset. Similar to the datset above, we will employ feature engineering to create variables calculating weekly change in the deaths and cases count. Since we are predicting for one week ahead, and we have to keep the same features as the traning dataset, the first week of the cases i.e Jan 22-29 gets dropped from the data. Once we have the dataset ready in the format same as the training data, we will make predictions using the XGB method and compare it with the actuals and calculate the Root Mean Square Error to evaluate the performance.  

```{r actual-data, echo = FALSE, message = FALSE, warning = FALSE}
actual_apr29 = read.csv("D:/Stat 542/Project/Apr29.csv")
actual_deaths = actual_apr29$X.Deaths_04.29.2020[-2055]
actual_apr29 = select(actual_apr29,-c(290:313))
actual_apr29 = select(actual_apr29,-c(180:197))
```

```{r actual-data-missing-values, echo = FALSE, message = FALSE, warning = FALSE}
# Pop_latitude and Pop_latitude almost identical to lat and long and dont have missing values
actual_apr29$lat = NULL
actual_apr29$lon = NULL

# For missing values in mortality column, replacing NA values with national average seems reasonable
actual_apr29$X3.YrMortalityAge.1Year2015.17[is.na(actual_apr29$X3.YrMortalityAge.1Year2015.17)] = mean(actual_apr29$X3.YrMortalityAge.1Year2015.17, na.rm = TRUE)

# For 7 counties missing eligible for care, national eligiblity mean % was applied
national_covered_mean = mean(
  actual_apr29$X.EligibleforMedicare2018 / actual_apr29$PopulationEstimate2018,
  na.rm = TRUE
)
actual_apr29$X.EligibleforMedicare2018[is.na(actual_apr29$X.EligibleforMedicare2018)] = national_covered_mean * actual_apr29$PopulationEstimate2018[is.na(actual_apr29$X.EligibleforMedicare2018)]

# For Medicare Enrollement Aged - all misisng values replaced with zero
actual_apr29$MedicareEnrollment.AgedTot2017[is.na(actual_apr29$MedicareEnrollment.AgedTot2017)] = 0

actual_apr29$DiabetesPercentage[is.na(actual_apr29$DiabetesPercentage)] = mean(actual_apr29$DiabetesPercentage, na.rm = TRUE)
actual_apr29$HeartDiseaseMortality[is.na(actual_apr29$HeartDiseaseMortality)] = mean(actual_apr29$HeartDiseaseMortality, na.rm = TRUE)
actual_apr29$StrokeMortality[is.na(actual_apr29$StrokeMortality)] = mean(actual_apr29$StrokeMortality, na.rm = TRUE)
actual_apr29$dem_to_rep_ratio[is.na(actual_apr29$dem_to_rep_ratio)] = mean(actual_apr29$dem_to_rep_ratio, na.rm = TRUE)

# Foreign travel ban column only has one value which we think represent the ban being implemented hence we can convert into a factor variable

actual_apr29$foreign.travel.ban = 1
actual_apr29$foreign.travel.ban = as.factor(actual_apr29$foreign.travel.ban)

# Assuming that missing values in stay-at-home column represent the stay-at-home order was not issued at all
actual_apr29$stay.at.home[is.na(actual_apr29$stay.at.home)] = 999999
actual_apr29$X.50.gatherings[is.na(actual_apr29$X.50.gatherings)] = 999999
actual_apr29$X.500.gatherings[is.na(actual_apr29$X.500.gatherings)] = 999999
actual_apr29$entertainment.gym[is.na(actual_apr29$entertainment.gym)] = 999999
# Too many missing values for columns, no reasonable way to get any info from them
actual_apr29$X3.YrMortalityAge1.4Years2015.17 = NULL
actual_apr29$X3.YrMortalityAge5.14Years2015.17 = NULL
actual_apr29$mortality2015.17Estimated = NULL

actual_apr29$SVIPercentile[is.na(actual_apr29$SVIPercentile)] = median(actual_apr29$SVIPercentile, na.rm = TRUE)
actual_apr29$HPSAShortage[is.na(actual_apr29$HPSAShortage)] = mean(actual_apr29$HPSAShortage, na.rm = TRUE)

# All counties with 3 Yr diabetes missing are very small in population so assuming 0 for those
actual_apr29$X3.YrDiabetes2015.17[is.na(actual_apr29$X3.YrDiabetes2015.17)] = 0


# Mortality rates age wise ar eimputed using random forest method in the mice package for imputation
set.seed(42)
imputed = mice(
  actual_apr29[, 64:71],
  method = "rf",
  m = 3,
  maxit = 3,
  printFlag = FALSE
)
completed_data = complete(imputed, 1)
actual_apr29[, 64:71] = completed_data
imputed = mice(
  actual_apr29[, 13:83],
  method = "rf",
  m = 3,
  maxit = 3,
  printFlag = FALSE
)
completed_data = complete(imputed, 1)
actual_apr29[, 13:83] = completed_data

actual_apr29 = actual_apr29[-2055, ]

rm(completed_data, imputed)

```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
reg_data = actual_apr29
target = reg_data$X.Deaths_04.22.2020
reg_data$last7daysdeaths = reg_data[, "X.Deaths_04.22.2020"] - reg_data[, "X.Deaths_04.15.2020"]
reg_data$prev7daysdeaths = reg_data[, "X.Deaths_04.15.2020"] - reg_data[, "X.Deaths_04.08.2020"]
reg_data$marchweek5 = reg_data[, "X.Deaths_04.08.2020"] - reg_data[, "X.Deaths_04.01.2020"]
reg_data$marchweek4 = reg_data[, "X.Deaths_04.01.2020"] - reg_data[, "X.Deaths_03.25.2020"]
reg_data$marchweek3 = reg_data[, "X.Deaths_03.25.2020"] - reg_data[, "X.Deaths_03.18.2020"]
reg_data$marchweek2 = reg_data[, "X.Deaths_03.18.2020"] - reg_data[, "X.Deaths_03.11.2020"]
reg_data$marchweek1 = reg_data[, "X.Deaths_03.11.2020"] - reg_data[, "X.Deaths_03.04.2020"]
reg_data$febweek4 = reg_data[, "X.Deaths_03.04.2020"] - reg_data[, "X.Deaths_02.26.2020"]
reg_data$febweek3 = reg_data[, "X.Deaths_02.26.2020"] - reg_data[, "X.Deaths_02.19.2020"]
reg_data$febweek2 = reg_data[, "X.Deaths_02.19.2020"] - reg_data[, "X.Deaths_02.12.2020"]
reg_data$febweek1 = reg_data[, "X.Deaths_02.12.2020"] - reg_data[, "X.Deaths_02.05.2020"]
reg_data$janweek4 = reg_data[, "X.Deaths_02.05.2020"] - reg_data[, "X.Deaths_01.29.2020"]
reg_data$last7dayscases = reg_data[, "X.Cases_04.22.2020"] - reg_data[, "X.Cases_04.15.2020"]
reg_data$prev7dayscases = reg_data[, "X.Cases_04.15.2020"] - reg_data[, "X.Cases_04.08.2020"]
reg_data$marchCases_week5 = reg_data[, "X.Cases_04.08.2020"] - reg_data[, "X.Cases_04.01.2020"]
reg_data$marchCases_week4 = reg_data[, "X.Cases_04.01.2020"] - reg_data[, "X.Cases_03.25.2020"]
reg_data$marchCases_week3 = reg_data[, "X.Cases_03.25.2020"] - reg_data[, "X.Cases_03.18.2020"]
reg_data$marchCases_week2 = reg_data[, "X.Cases_03.18.2020"] - reg_data[, "X.Cases_03.11.2020"]
reg_data$marchCases_week1 = reg_data[, "X.Cases_03.11.2020"] - reg_data[, "X.Cases_03.04.2020"]
reg_data$febCases_week4 = reg_data[, "X.Cases_03.04.2020"] - reg_data[, "X.Cases_02.26.2020"]
reg_data$febCases_week3 = reg_data[, "X.Cases_02.26.2020"] - reg_data[, "X.Cases_02.19.2020"]
reg_data$febCases_week2 = reg_data[, "X.Cases_02.19.2020"] - reg_data[, "X.Cases_02.12.2020"]
reg_data$febCases_week1 = reg_data[, "X.Cases_02.12.2020"] - reg_data[, "X.Cases_02.05.2020"]
reg_data$janCases_week4 = reg_data[, "X.Cases_02.05.2020"] - reg_data[, "X.Cases_01.29.2020"]
reg_data = select(reg_data,-c(83:266))

# Removing variables which like serial numbers etc and total deaths
reg_data$X = NULL
reg_data$StateName = NULL
reg_data$countyFIPS = NULL
reg_data$CountyName = NULL
reg_data$Rural.UrbanContinuumCode2013 = as.factor(reg_data$Rural.UrbanContinuumCode2013)

# Foregin travel ban only has one level
reg_data$foreign.travel.ban = NULL

# Federal guideline has no variation
reg_data$federal.guidelines = NULL
reg_data$State = NULL

```

```{r future-pred, echo = FALSE, message = FALSE, warning = FALSE}

data_test = reg_data
set.seed(42)
pred_29april = predict(xgb_train_1, data_test)
total_deaths_apr29 = pred_29april + target
pred_data = tibble("County_FIPS" = actual_apr29$countyFIPS,
                   "total_deaths_Apr29" = total_deaths_apr29)
pred_data$total_deaths_Apr29 = ceiling(pred_data$total_deaths_Apr29)
pred_data$lastweekdeaths = ceiling(pred_29april)
pred_data$deathsuptoApr22 = target
pred_data$Actual_deaths_Apr29 = actual_deaths
rmse_actual = sqrt(mean((pred_data$Actual_deaths_Apr29 - pred_data$total_deaths_Apr29) ^
                          2
))
plot(
  log(pred_data$Actual_deaths_Apr29),
  log(pred_data$total_deaths_Apr29),
  main = "Log of Actuals vs Predicted Deaths Till April 29",
  xlab = "Actual Deaths",
  ylab = "Predicted Deaths",
  col = "deepskyblue",
  sub = "Logarithmic Scale",
  pch = 19,
  cex = 1
)
abline(0, 1, col = "darkorange", lty = 1, lwd = 3)
```

The Root Mean Sqaure Error of our predictions as compared to the actual deaths till April 29 is `r rmse_actual` which is pretty good in the context of the situation. In the graph above, we can see that model predictions are more inaccurate for the counties with very low death counts but is relatively much more accurate with counties with more deaths, this is a promising sign because if we can predict accurately the death count in more heavily hit counties, we can then focus on those and take necessary steps to combat the expected rise. 


# Appendix

```{r visuals}
grid.arrange(p01, p02, p03, p04, p05, p06, nrow = 3, ncol = 2)
```


```{r}
knitr::knit_exit()
```

