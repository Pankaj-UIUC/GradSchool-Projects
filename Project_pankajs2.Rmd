---
title: "STAT542 Project : A study on COVID-19 spread across US Counties"
author: "Pankaj Sharma(pankajs2), Aastha Nargas(anargas2)"
date: "5/14/2020"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
header-includes: 
  \usepackage{dcolumn}
  \usepackage{colortbl}
---

# Team

- Aastha Nargas(anargas2): Team Lead
- Pankaj Sharma(pankajs2)

\newpage

# Project Description

The novel coronavirus, COVID-19, has altered lives across the globe, damaging social and economic structures in its wake. At the time of collating the final draft of this project, though the number of cases has been increasing in the US daily, the rate of increase now appears to be dropping in the worst hit areas, however, there is still a long way to go before we reach a new normal.  This project is our attempt to use our statistical tool kit and understand the spread of COVID-19 across counties in the United States. 

The main aim of this project is to model and predict the number of deaths at a county-level, over the span of a week, using data from January 22, 2020 to April 22, 2020. We have also focused on recognizing underlying patterns of the spread of this virus. By comprehending the factors which led to higher mortality rates some light can be thrown on how to tackle the virus from a governance point of view. 

We employ both unsupervised and supervised learning techniques to further our cause. Clustering techniques like K-Means, Divisive Hierarchical and Self-Organizing Maps highlight the contrast between worst hit counties and the rest. Random Forest and Gradient Boosting methods are used to classify and predict the counties with death per 100,000 population, with the models proving to be 83% accurate and bring attention to variables which are important for classification. For regression task, we work with Random Forest, Gradient Boosting and Penalized Linear Regression, which results in the root mean squared error ranging between 7% to 12%. 

Our analysis provides us with insights about the spread of COVID-19 and the factors driving the menace. However, due to the dynamic and ever-evolving nature of COVID-19, these results should be understood in the perspective of the geographical region for which they are modeled and may not perform well for other regions. Also, with governments across the globe trying various methods to tackle the spread, these models are also dependent on social distancing and policy factors. 

# Literature Review

The coronavirus disease,which originated in Wuhan,China in December 2019 has becomes global pandemic infecting over 4 million people across the world. The disease which has spread exponentially over the last 5 months has scientists and researchers studying its various aspects from biologists and epidemiologists to data scientists and statisticians. 
This project takes data from the data repository created by Professor Bin Yu's research group[^1]. Their paper[^2] aims to forecast short-term COVID-19 mortality at the county-level in the United States using a Combined Linear and Exponential Predictors ensemble approach using data from January 22, 2020 to April 8, 2020. Out of the worst affected 6 counties that the paper focuses on, three of had closer predictions than the others since they continued to exhibit an exponential growth in the number of deaths.

A paper by S. Zhang et al[^3] attempts to estimate the reproductive number of COVID-19 on the ship Diamond Princess and predict the number of new cases daily. The authors used the "earlyR" package in R to estimate the reproductive cases(R0) and "projection" package to simulate the cumulative epidemic trajectories and future daily cases. The early stage median of R0 was around 2.28 and the future daily increase was dependent on the change in R0 value. Another study estimates the reproduction number for epidemic in Japan by using least square based method with Poisson noise and then apply SEIR compartmental model for the prediction of the peak of epidemic.[^4]

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
set.seed(42)
```

```{r, warning=FALSE, message=FALSE, echo = FALSE}
library(caret)
library(tidyverse)
library(rsample)
library(kableExtra)
library(mice)
library(randomForest)
library(gbm)
library(NbClust)
library(ggplot2)
library(ggthemes)
library(viridis)
library(cluster)
library(maps)
library(usmap)
library(FactoMineR)
library(factoextra)
library(kohonen)
library(urbnmapr)
library(xgboost)
library(ggcorrplot)
library(reshape2)
```

```{r load-data, echo = FALSE}
covid_data = read.csv("https://teazrq.github.io/stat542/data/county_data_apr22.csv")
```

# Data

The data is downloaded from [Github data](https://github.com/Yu-Group/covid19-severity-prediction) processed and constructed by Prof. Bin Yu's group at Berkeley. It contains county level information about the cases and deaths due to Covid-19 and also includes demographic information about population groups and hospital information. However before doing any modelling on the data, we need to properly handle the missing data. To start with we have two sets of columns of `Latitude` and `Longitude`, since both are almost the same, we will only keep the ones showing population centers. For columns which have too many rows missing data like `MortalityAge1.4Years2015.17, MortalityAge5.14Years2015.17,mortality2015.17Estimated`, we simply delete the columns as there is not enough information to do any imputation.

For columns like `Diabetes Percentage, Stroke Mortality, and Heart Disease Mortality` it makes sense to replace the missing values with mean since these are continuous variables and there are not many missing values. For the columns related to `Stay at Home`, it is given the the values represent the day number on which the orders were issued. It is reasonable to assume that for the counties that are missing values in these columns, there may not have been an official Stay at Home order issued so there is no day count. Hence, one specific value is assigned to these columns to represent the missing Stay at Home order. For `Medicare Enrolled`, national mean percentage of covered was used to calculate the eligible using the population of the county. 

For other columns left with missing values, imputation was run using the `Mice` package in R and using `Random Forest` for imputation. `Random Forest` is chosen as it is one of the best methods to make sure the variables don't become too inter-correlated and hence will make them more useful for modelling later. 

```{r missing-values, warning = FALSE, message = FALSE, echo = FALSE}
# Pop_latitude and Pop_latitude almost identical to lat and long and dont have missing values
covid_data$lat = NULL
covid_data$lon = NULL

# For missing values in mortality column, replacing NA values with national average seems reasonable
covid_data$X3.YrMortalityAge.1Year2015.17[is.na(covid_data$X3.YrMortalityAge.1Year2015.17)] = mean(covid_data$X3.YrMortalityAge.1Year2015.17, na.rm = TRUE)

# For 7 counties missing eligible for care, national eligiblity mean % was applied
national_covered_mean = mean(covid_data$X.EligibleforMedicare2018 / covid_data$PopulationEstimate2018,
                             na.rm = TRUE)
covid_data$X.EligibleforMedicare2018[is.na(covid_data$X.EligibleforMedicare2018)] = national_covered_mean * covid_data$PopulationEstimate2018[is.na(covid_data$X.EligibleforMedicare2018)]

# For Medicare Enrollement Aged - all misisng values replaced with zero
covid_data$MedicareEnrollment.AgedTot2017[is.na(covid_data$MedicareEnrollment.AgedTot2017)] = 0

covid_data$DiabetesPercentage[is.na(covid_data$DiabetesPercentage)] = mean(covid_data$DiabetesPercentage, na.rm = TRUE)
covid_data$HeartDiseaseMortality[is.na(covid_data$HeartDiseaseMortality)] = mean(covid_data$HeartDiseaseMortality, na.rm = TRUE)
covid_data$StrokeMortality[is.na(covid_data$StrokeMortality)] = mean(covid_data$StrokeMortality, na.rm = TRUE)
covid_data$dem_to_rep_ratio[is.na(covid_data$dem_to_rep_ratio)] = mean(covid_data$dem_to_rep_ratio, na.rm = TRUE)

# Foreign travel ban column only has one value which we think represent the ban being implemented hence we can convert into a factor variable

covid_data$foreign.travel.ban = 1
covid_data$foreign.travel.ban = as.factor(covid_data$foreign.travel.ban)

# Assuming that missing values in stay-at-home column represent the stay-at-home order was not issued at all
covid_data$stay.at.home[is.na(covid_data$stay.at.home)] = 999999
covid_data$X.50.gatherings[is.na(covid_data$X.50.gatherings)] = 999999
covid_data$X.500.gatherings[is.na(covid_data$X.500.gatherings)] = 999999
covid_data$entertainment.gym[is.na(covid_data$entertainment.gym)] = 999999
# Too many missing values for columns, no reasonable way to get any info from them
covid_data$X3.YrMortalityAge1.4Years2015.17 = NULL
covid_data$X3.YrMortalityAge5.14Years2015.17 = NULL
covid_data$mortality2015.17Estimated = NULL

covid_data$SVIPercentile[is.na(covid_data$SVIPercentile)] = median(covid_data$SVIPercentile, na.rm = TRUE)
covid_data$HPSAShortage[is.na(covid_data$HPSAShortage)] = mean(covid_data$HPSAShortage, na.rm = TRUE)

# All counties with 3 Yr diabetes missing are very small in population so assuming 0 for those
covid_data$X3.YrDiabetes2015.17[is.na(covid_data$X3.YrDiabetes2015.17)] = 0

# Mortality rates age wise ar eimputed using random forest method in the mice package for imputation
set.seed(42)
imputed = mice(
  covid_data[, 64:71],
  method = "rf",
  m = 3,
  maxit = 3,
  printFlag = FALSE
)
completed_data = complete(imputed, 1)
covid_data[, 64:71] = completed_data
imputed = mice(
  covid_data[, 13:83],
  method = "rf",
  m = 3,
  maxit = 3,
  printFlag = FALSE
)
completed_data = complete(imputed, 1)
covid_data[, 13:83] = completed_data

missing_values = as.data.frame(sapply(covid_data, function(x)
  sum(is.na(x))))
df <- cbind(var_name = rownames(missing_values), missing_values)
rownames(df) <- 1:nrow(df)
df = df %>%
  rename(missing_count = "sapply(covid_data, function(x) sum(is.na(x)))")
#df[df$missing_count > 0, ]

rm(completed_data, imputed, missing_values, df)

```

# Unsupervised Learning

## Clustering

Clustering is a technique which groups observations which have similar characteristics in a data-set. The observations which belong to a particular group are closer in nature to each other than those from others groups. This helps in the exploratory phase of the analyzing the data and finding hidden patterns.  

Our aim is to cluster counties which show resembling patterns based on demographics and health indicators. We use three clustering methods to find answers from within the data. 

- K-Means Clustering
- Hierarchical Clustering with PCA
- Self Organizing Maps

### K-Means Clustering

One of the most widely used method of clustering, K-Means is an iterative method to assign observations to clusters. K-Means algorithm groups counties into different clusters based on population density, population demographic, health status of county and health care quality.

We perform K- Means clustering on select variables such as : `PopulationDensityperSqMile2010`, `CensusPopulation2010`, `MedianAge2010`, `MedicareEnrollment.AgedTot2017`, `StrokeMortality`, `X.Hospitals`, `HPSAShortage`, `HeartDiseaseMortality`, `Smokers_Percentage`, `RespMortalityRate2014`, `X.ICU_beds`, `SVIPercentile`, `dem_to_rep_ratio`
 
Since, K-Means require an initial number of cluster, we use the Elbow-Method (\autoref{mylabel}) to determine optimal number of clusters. This approach tests a range of number of clusters calculating the sum of squared errors against each value. We have chosen the K value 5, at which the increase in clusters causes a small decrease in error. K-Means clustering for our data creates 5 clusters with the highest number of counties lying in cluster 3. (\autoref{mylabel3})

```{r k-means, echo = FALSE, fig.align = "center", fig.height = 6, fig.width = 12}

# Creating a subset of variables which are needed
clust_data = covid_data[, c(18, 19, 20, 22, 25, 26, 27, 28, 32, 33, 34, 80, 81)]

# to get the optimal number of clusters we use elbow method
clust_data_mat = as.matrix(clust_data)

## Elbow method
wss = (nrow(clust_data_mat) - 1) * sum(apply(clust_data_mat, 2, var))
  for (i in 2:30) wss[i] = sum(kmeans(clust_data_mat,
                                       centers = i, iter.max = 30)$withinss)

p01 = ggplot(as.data.frame(cbind(wss, seq(1,30))), aes(x = V2, y = wss)) +
  geom_line(linetype = "dashed") +
  geom_point() + 
  ggtitle("Fig 1 - Within groups Sum of Square vs Clusters") +
  xlab("Number of Clusters") + ylab("Within groups sum of squares")

set.seed(1)
kmeans_cluster = kmeans(clust_data_mat, centers = 5, iter.max = 30)

#Binding clusters and base dataset
covid_data_test = cbind(covid_data, kmeans_cluster$cluster)
names(covid_data_test)[names(covid_data_test) == "kmeans_cluster$cluster"] = "cluster"

kmeans_cluster_table = tibble(
  "Cluster" = c("1", "2", "3", "4", "5") ,
  "Number of Counties" = c(table(covid_data_test$cluster)[1],
                           table(covid_data_test$cluster)[2],
                           table(covid_data_test$cluster)[3],
                           table(covid_data_test$cluster)[4],
                           table(covid_data_test$cluster)[5])
)

kable(kmeans_cluster_table, digits = 5,
      caption = "Table: K-Means Clusters",) %>%
  kable_styling("striped", full_width = FALSE, 
                latex_options = c("striped", "hold_position")) %>%
  column_spec(column = 1, bold = TRUE)


## Plot
df = counties
df$county_fips = as.integer(df$county_fips)

county_map = left_join(covid_data_test, df, by = c("countyFIPS" = "county_fips"))

p02 = county_map %>%
  ggplot(aes(long, lat, group = group, fill = cluster)) +
  geom_polygon(aes(fill = cluster), color = "black") +
  scale_fill_viridis_c(option = "C") +
  #ggtitle("Fig 2 - K-Means clustering of US Counties") +
  xlab("longitude") + ylab("latitude")

```

We now analyse the clusters and their underlying pattern. (\autoref{mylabel2})

We first analyse the ratio of death to populations across counties to see which clusters have the maximum deaths with respect to the populations. It is observed that clusters 2 and 5 have the highest death to population ratio, and we further deep dive into details of these clusters. 

- The maximum number of deaths have been in Cluster 5 and the number of confirmed cases being 4 in every 10000 people
- The most number of counties in Clusters 2 and 5 belong to the states California, New York and Texas
- Cluster 3 has maximum number of counties since these are the ones across US which are not as badly hit by the virus as the those in Cluster 2 and 5
- The cluster with highest number of deaths are performing worst in terms of ICU beds to total confirmed cases, this is a very important factor highlighting that the healthcare facilities were not up-to mark in these counties which resulted in more deaths. 

```{r k-means-analysis, echo = FALSE, fig.align = "center", fig.height = 6, fig.width = 12}
cluster_level_data = covid_data_test %>%
  group_by(cluster) %>%
  summarise(number = n(),
            population = sum(CensusPopulation2010),
            deaths = sum(tot_deaths),
            confirmed_casses = sum(tot_cases),
            icu_beds = sum(X.ICU_beds),
            stroke_mort = mean(StrokeMortality),
            mean_median_age = mean(MedianAge2010),
            sum_hospitals = sum(X.Hospitals),
            avg_svi = mean(SVIPercentile),
            avg_diab = mean(DiabetesPercentage),
            pop_65above = sum(PopulationEstimate65.2017),
            sum_mort_65_74 = sum(X3.YrMortalityAge65.74Years2015.17),
            avg_mort_65_74 = mean(X3.YrMortalityAge65.74Years2015.17))

# Deaths/Population
cluster_level_data$death_ratio = (cluster_level_data$deaths/cluster_level_data$population)*100

p03 = ggplot(data = cluster_level_data, aes(x = cluster, y = death_ratio)) +
  geom_bar(stat = "identity", fill = "deepskyblue") + 
  ggtitle("Fig 3.a - Death to population ratio across clusters") +
  xlab("Clusters") + ylab("Death/Population")


# Cases/Population
cluster_level_data$case_ratio = (cluster_level_data$confirmed_casses/cluster_level_data$population)*100

p04 = ggplot(data = cluster_level_data, aes(x = cluster, y = case_ratio)) +
  geom_bar(stat = "identity", fill = "deepskyblue") + 
  ggtitle("Fig 3.b - Cases to population ratio across clusters") +
  xlab("Clusters") + ylab("Cases/Population")

#States/Regions
kmeans_subset_data = subset(covid_data_test, cluster == 2 | cluster == 5)

kmeans_subset_data2 = kmeans_subset_data %>%
  group_by(State) %>%
  summarise(number = n())
kmeans_subset_data2$State = as.character(kmeans_subset_data2$State)
kmeans_subset_data2[1, 1] = as.character("Hawaii")
kmeans_subset_data2 = kmeans_subset_data2[order(-kmeans_subset_data2$number),]

state_region_split = tibble(
  "State" = kmeans_subset_data2$State ,
  "Number of Counties" = c(kmeans_subset_data2$number)
  )
kable(state_region_split, digits = 5,
      caption = "Number of Counties across States - Cluster 2 and 5(K-Means)") %>%
  kable_styling("striped", full_width = FALSE,
                latex_options = c("striped", "hold_position")) %>%
  column_spec(column = 1, bold = TRUE)

#ICU beds/Confirmed Cases
cluster_level_data$icu_beds_pop = (cluster_level_data$icu_beds/cluster_level_data$confirmed_casses)*100
p05 = ggplot(data = cluster_level_data, aes(x = cluster, y = icu_beds_pop)) +
  geom_bar(stat = "identity", fill = "deepskyblue") + 
  ggtitle("Fig 3.c - ICU Beds to Cases ratio across clusters") +
  xlab("Clusters") + ylab("ICU Beds/Confirmed Cases")

#SVI
p06 = ggplot(data = cluster_level_data, aes(x = cluster, y = avg_svi)) +
  geom_bar(stat = "identity", fill = "deepskyblue") + 
  ggtitle("Fig 3.d - Average Percentile of Social Vulnerability Index across clusters") +
  xlab("Clusters") + ylab("Avg Percentile SVI")

cluster_level_data$mort_65_74_ratio = (cluster_level_data$sum_mort_65_74/cluster_level_data$population)*100

#Counties with highest population above 55 for population age 65-74
covid_data_test$pop_over_55 = rowSums(cbind(covid_data_test[, c(53:62)]))
##covid_data_test$ratio_pop_over_55 = (covid_data_test$pop_over_55/covid_data_test$CensusPopulation2010) 
#population over 50/total county population
top50_counties = covid_data_test[order(-covid_data_test$pop_over_55),]
top50_counties = top50_counties[1:50, c(5, 6, 272, 273)]
M = as.matrix(table(top50_counties$cluster))
M = as.data.frame(cbind(rownames(M), M))

p07 = ggplot(data = M, aes(x = V1, y = V2)) +
  geom_bar(stat = "identity", fill = "deepskyblue") + 
  ggtitle("Fig 3.e - Spread of Top 50 Counties with Population above 55 across clusters") +
  xlab("Clusters") + ylab("Counties")

#HPSA Shortage
top20_counties = covid_data_test[order(-covid_data_test$HPSAUnderservedPop),]
top20_counties = top20_counties[1:20, c(5, 6, 83, 270, 271, 272)]

M = as.matrix(table(top20_counties$cluster))
M = as.data.frame(cbind(rownames(M), M))
M$V2 = as.numeric(as.character(M$V2))
p08 = ggplot(data = M, aes(x = V1, y = V2)) +
  geom_bar(stat = "identity", fill = "deepskyblue") + 
  ggtitle("Fig 3.f - Counties with highest HPSA underserved population across clusters") +
  xlab("Clusters") + ylab("Counties") +
  scale_y_continuous(breaks=c(0, 2, 4, 6, 8, 10, 12))


```

### Hierarchical Clustering and PCA

In order to get better clustering results, we use the help of PCA method to extract variables which are important and explain the variation in the data. We begin with prepping the data by sub-setting variables based on judgement and requirement of PCA, i.e numerical and non-zero variables.

We narrowed down 24 variables contributing the most to 1st and 2nd PCA dimensions and use those for hierarchical clustering.(\autoref{mylabel5})

Hierarchical clustering  has two types, agglomerative and divisive. We perform Divisive Hierarchical clustering, using the function `diana` which is a top-down approach. In this approach all the observations are assigned a single cluster and then are further partitioned into similar clusters until each observation has a cluster.(\autoref{mylabel4})

We get 8 clusters based on hierarchical clustering and we further dig deeper into these clusters. 

```{r hc, echo = FALSE, fig.align = "center", fig.height = 6, fig.width = 12, message = FALSE, warning = FALSE}

#HClust and PCA
## Doing data prep, excluding factor variables and 0 variance variables
pca_data = covid_data[,-c(as.integer(which(sapply(
  covid_data, is.factor
))))]
pca_data = pca_data[, which(apply(pca_data, 2, var) != 0)]
pca_data = pca_data[,-c(77:245)]

#PCA
pca_results = PCA(pca_data, scale.unit = TRUE, graph = FALSE)

#Choosing Variables for Hierachical Clustering
#Variables
vars = get_pca_var(pca_results)
M = as.matrix(as.matrix(sort(vars$contrib[, 1], decreasing = TRUE))[c(1:20), ])
M2 = as.matrix(as.matrix(sort(vars$contrib[, 2], decreasing = TRUE))[c(1, 2, 5, 6),])
variables_hclust = (c(row.names(M), row.names(M2)))

#HClust
hclust_data = covid_data[, c(variables_hclust)]
hclust_data = scale(hclust_data)

#Divisive Hierarchial Clustering

hclusters = diana(hclust_data)
clusters = cutree(hclusters, k = 8)

#Merging with main data set
hclust_covid_data = as.data.frame(covid_data %>% mutate(cluster = clusters))
hclusters_table = tibble(
  "Cluster" = c("1", "2", "3", "4", "5", "6", "7", "8") ,
  "Number of Counties" = c(
    table(hclust_covid_data$cluster)[1],
    table(hclust_covid_data$cluster)[2],
    table(hclust_covid_data$cluster)[3],
    table(hclust_covid_data$cluster)[4],
    table(hclust_covid_data$cluster)[5],
    table(hclust_covid_data$cluster)[6],
    table(hclust_covid_data$cluster)[7],
    table(hclust_covid_data$cluster)[8]
  )
)

kable(hclusters_table, digits = 5,
      caption = "Hierarchical Clusters") %>%
  kable_styling("striped", full_width = FALSE,
                latex_options = c("striped", "hold_position")) %>%
  column_spec(column = 1, bold = TRUE)

#
df = counties
df$county_fips = as.integer(df$county_fips)

county_map = left_join(hclust_covid_data, df, by = c("countyFIPS" = "county_fips"))

p09 = county_map %>%
  ggplot(aes(long, lat, group = group, fill = cluster)) +
  geom_polygon(aes(fill = cluster), color = "black") +
  scale_fill_viridis_c(option = "C") +
  #ggtitle("Fig 7 - Hierarchical clustering of US Counties") +
  xlab("longitude") + ylab("latitude")

```

### Self Organizing Maps

Self-Organizing Map or SOM is a neural network clustering algorithm which helps in visualizing a high-dimension data in a lower dimensional space. We have used the function `som` from `kohonen package`

We select the same variables which we used for K-Means clustering focusing demographics and health factors across counties. The som algorithm is then run on the scaled data of counties. 

The nodes in the rectangular topological figure are mapped to each county and have representation of the variables in the data. Similar input samples are mapped to nodes in the same area. 

We have two plots to understand the SOM further, the codes plot shows the weight of each variable in a particular node and the counts plot represents the number of counties in each node.(\autoref{mylabel6})

```{r som, echo = FALSE, message = FALSE, warning = FALSE}
som_data = covid_data[, c(18, 19, 20, 22, 25, 26, 27, 28, 32, 33, 80, 81)]
som_data = scale(som_data)

set.seed(1)
g  = somgrid(4, 4 , "rectangular")
map = som(som_data, grid = g)
SOM_clusters = as.data.frame(cbind(
  Clusters = c(1:16),
  "No. of counties" = (table(map$unit.classif))
))
```

# Supervised Learning

For supervised learning, we focus on both classification and regression tasks. 

## Classification

For classification, we first create the response variable using `tot_deaths/PopulationEstimate2018`. We split the processed data after doing the imputation into train and test data-sets. We will use the training data-set to train our models and use the testing data-set to make predictions and check for accuracy. To tune the hyper-parameters for the models, we will use `cross-validation`, for the purposes of modelling, we are using the inbuilt functions in the `caret` library. Some of the columns which are just serial numbers or columns which don't contain discriminating information are removed before training the model on the data. 

We have considered the following models for the purposes of the classification:

- Random Forest: We used the inbuilt `train` function in the `caret` library with method set to `rf`.

  - Hyperparameter `Mtry` was tuned using 5-fold cross validation with a tunelength of 15, 
    The final value used for the model was mtry = 54.

- Gradient Boosting Method: We used the inbuilt `train` function in the `caret` library with method set to `gbm`

  - Hyperparameters `ntrees, shrinkage, interaction.depth` and `n.minobsinnode` were tuned using 5-fold cross-validation.

```{r test-train-class, echo = FALSE, warning = FALSE, message = FALSE}
# making a copy of the data for classification
class_data = covid_data

# Removing daily cases and deaths since we have the total cases and deaths variable
class_data[, 84:269] = NULL

# creating the response variable
class_data$dpt = as.factor(ifelse((class_data$tot_deaths / class_data$PopulationEstimate2018) *
                                    100000 > 1,
                                  "pos",
                                  "neg"
))

#table(class_data$dpt)

# Removing variables which like serial numbers etc and total deaths
class_data$X = NULL
class_data$tot_deaths = NULL
class_data$StateName = NULL
class_data$countyFIPS = NULL
class_data$CountyName = NULL
class_data$Rural.UrbanContinuumCode2013 = as.factor(class_data$Rural.UrbanContinuumCode2013)

# Foregin travel ban only has one level
class_data$foreign.travel.ban = NULL

# Federal guideline has no variation
class_data$federal.guidelines = NULL
class_data$State = NULL

set.seed(42)
split = initial_split(class_data, prop = 0.80)
train = training(split)
test = testing(split)
```

### Random Forest

```{r, cv-control-rf, echo = FALSE}
cv_control = trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)
```

```{r, rf, echo = FALSE, message = FALSE, cache = TRUE}
set.seed(42)
mod_rf = train(
  form = dpt ~ .,
  data = train,
  method = "rf",
  trControl = cv_control,
  metric = 'ROC',
  tuneLength = 15,
  verbose = FALSE
)
mod_rf

```

### Gradient Boosting Method

```{r, cv-control-gbm, echo = FALSE}
cv_control = trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

grid_gbm <-
  expand.grid(
    n.trees = c(100, 500, 1000),
    shrinkage = c(0.01, 0.05, 0.1, 0.5),
    n.minobsinnode = c(3, 5, 10),
    interaction.depth = c(1, 5, 10)
  )
```

```{r, gbm, echo = FALSE, message = FALSE, cache = TRUE}
set.seed(42)
mod_gbm = train(
  form = dpt ~ .,
  data = train,
  method = "gbm",
  trControl = cv_control,
  metric = 'ROC',
  tuneLength = 10,
  tuneGrid = grid_gbm,
  verbose = FALSE
)

```

The final values used for the model were n.trees = 100, interaction.depth = 5, shrinkage = 0.01 and n.minobsinnode = 5.

Based on the ROC, best model out of the two tuned, Random Forest and Gradient Boosting, the Gradient Boosting model with the tuned hyper-parameters was selected to make predictions on the test data. Performance metrics of the model on the test data are shown below.

```{r results-class, echo = FALSE}
result = tibble(
  "Model Name" = c("Random Forest",
                   "Gradient Boosting Method") ,
  "ROC" = c(max(mod_rf$results[2]),
            max(mod_gbm$results[5])),
  "Sensitivity" = c(max(mod_rf$results[3]),
                    max(mod_gbm$results[6]))
)

kable(result, digits = 5,
      caption = "Cross Validated ROC for Random Forest and Gradient Boosting") %>%
  kable_styling(
    c("striped", "bordered", "hover"),
    full_width = FALSE,
    latex_options = c("striped", "hold_position")
  ) %>%
  column_spec(column = 1, bold = TRUE)
```

Confusion matrix to see the performance metrics of the GBM classification model on test data:

```{r test-pred, echo = FALSE}
confusionMatrix(predict(mod_gbm, test, type = "raw"),
                reference = test$dpt,
                positive = "pos")
```

Using the selected model on the testing data, we are able to achieve an accuracy of nearly 83% which is pretty good in this context. Using these models, we also checked to see which variables are most effective in making accurate classifications. In the tables below, we see the top-10 variables ordered by their importance in the classification for both the Gradient Boosting Method and Random Forest Method. Total cases as expected is the most important variable in both the models as it is directly related to the total number of deaths. `Heart Disease Mortality` and `Daibetes Percentage` seem to important too indicating that more vulnerable population is effected more and is very discriminating in making the predictions. 
In the GBM model, `Stay at Home` and healthcare factors are very important in making the accurate classifications indicating that the places where the healthcare system wasn't overwhelmed had less deaths as compared to other counties. Another important factor is `Population Density` which relate to the $R_{0}$ factor of the infection. This information in conjunction with the domain knowledge of the virus can be used to take actions in certain areas and reduce the death count.

```{r varimp-gbm, echo = FALSE}
gbmimp <- varImp(mod_gbm, scale = FALSE)
gbmimp = as.data.frame(gbmimp$importance)
gbmimp <- cbind(var_name = rownames(gbmimp), gbmimp)
rownames(gbmimp) <- 1:nrow(gbmimp)
gbmimp = gbmimp[gbmimp$Overall > 0,]
gbmimp = gbmimp[order(gbmimp$Overall, decreasing = TRUE), ]

rf_imp <- varImp(mod_rf, scale = FALSE)
rf_imp = as.data.frame(rf_imp$importance)
rf_imp <- cbind(var_name = rownames(rf_imp), rf_imp)
rownames(rf_imp) <- 1:nrow(rf_imp)
rf_imp = rf_imp[rf_imp$Overall > 0,]
rf_imp = rf_imp[order(rf_imp$Overall, decreasing = TRUE), ]
```

```{r varimpvis, echo = FALSE, warning = FALSE, message = FALSE}
kable(gbmimp[1:10, ],
      digits = 5,
      caption = "Variable Importance for prediction(GBM Model)",
      align = "clc") %>%
  kable_styling(
    c("striped", "bordered", "hover"),
    full_width = FALSE,
    latex_options = c("striped", "hold_position")
  ) %>%
  column_spec(column = 1, bold = TRUE)

kable(rf_imp[1:10, ],
      digits = 5,
      caption = "Variable Importance for prediction(RF Model)",
      align = "clc") %>%
  kable_styling(
    c("striped", "bordered", "hover"),
    full_width = FALSE,
    latex_options = c("striped", "hold_position")
  ) %>%
  column_spec(column = 1, bold = TRUE)
```

\newpage

## Regression

To predict the deaths one week ahead, we need to capture the change occurring in the deaths and cases as we move across time. Instead of taking a time series approach, we approached this using feature engineering. Also, Instead of predicting total deaths after one week into the future, we will use our models to predict the deaths occurred in the week after the data ends and then will add the given deaths till date to the predicted values to get the total deaths by the end of the week. To achieve this, we first created variables calculating the change in death and cases every 7 days as we move back from the last day data contained in the data-set. This allowed us to attain weekly change in cases and deaths and accounted for the change in cases/deaths or we can call it slope as well. Again we removed the variables like Serial Numbers and other variables which had no variation in them throughout the data-set. Once we had the data-set ready, we split it into training and testing data-sets. We will train the following three models on the training data:

- Random Forest: We used the inbuilt `train` function in the `caret` library with method set to `rf`.

  - Hyper-parameter `Mtry` was tuned using 5-fold cross validation with 3 repeats with a tune-length of 10
  
- Penalized Linear Regression: `alpha` which decides if the penalty applied is l1 or l2 and `lambda` the regularization parameter were tuned using 
5-fold cross validation in the `train` function in the `caret` library using `glmnet` method.

- Xtreme Gradient Boosting: `eta` , `max_depth` and `colsample_bytree` were tuned using 5-fold cross-validation, rest of the hyper-parameters were kept constant.

Best tune parameters for all the three methods is shown below. Along with that, we have also shown the minimum Root Mean Square Error achieved with each mode. 

```{r reg-data, echo = FALSE, warning = FALSE, message = FALSE}
reg_data = covid_data
target = reg_data$tot_deaths
reg_data$deathscurrentweek = reg_data[, "X.Deaths_04.22.2020"] - reg_data[, "X.Deaths_04.15.2020"]
reg_data$deaths = NULL
reg_data$cases = NULL
reg_data = select(reg_data,-c(261:269))
reg_data$last7daysdeaths = reg_data[, "X.Deaths_04.15.2020"] - reg_data[, "X.Deaths_04.08.2020"]
reg_data$prev7daysdeaths = reg_data[, "X.Deaths_04.08.2020"] - reg_data[, "X.Deaths_04.01.2020"]
reg_data$marchweek5 = reg_data[, "X.Deaths_04.01.2020"] - reg_data[, "X.Deaths_03.25.2020"]
reg_data$marchweek4 = reg_data[, "X.Deaths_03.25.2020"] - reg_data[, "X.Deaths_03.18.2020"]
reg_data$marchweek3 = reg_data[, "X.Deaths_03.18.2020"] - reg_data[, "X.Deaths_03.11.2020"]
reg_data$marchweek2 = reg_data[, "X.Deaths_03.11.2020"] - reg_data[, "X.Deaths_03.04.2020"]
reg_data$marchweek1 = reg_data[, "X.Deaths_03.04.2020"] - reg_data[, "X.Deaths_02.26.2020"]
reg_data$febweek4 = reg_data[, "X.Deaths_02.26.2020"] - reg_data[, "X.Deaths_02.19.2020"]
reg_data$febweek3 = reg_data[, "X.Deaths_02.19.2020"] - reg_data[, "X.Deaths_02.12.2020"]
reg_data$febweek2 = reg_data[, "X.Deaths_02.12.2020"] - reg_data[, "X.Deaths_02.05.2020"]
reg_data$febweek1 = reg_data[, "X.Deaths_02.05.2020"] - reg_data[, "X.Deaths_01.29.2020"]
reg_data$janweek4 = reg_data[, "X.Deaths_01.29.2020"] - reg_data[, "X.Deaths_01.22.2020"]
reg_data$last7dayscases = reg_data[, "X.Cases_04.15.2020"] - reg_data[, "X.Cases_04.08.2020"]
reg_data$prev7dayscases = reg_data[, "X.Cases_04.08.2020"] - reg_data[, "X.Cases_04.01.2020"]
reg_data$marchCases_week5 = reg_data[, "X.Cases_04.01.2020"] - reg_data[, "X.Cases_03.25.2020"]
reg_data$marchCases_week4 = reg_data[, "X.Cases_03.25.2020"] - reg_data[, "X.Cases_03.18.2020"]
reg_data$marchCases_week3 = reg_data[, "X.Cases_03.18.2020"] - reg_data[, "X.Cases_03.11.2020"]
reg_data$marchCases_week2 = reg_data[, "X.Cases_03.11.2020"] - reg_data[, "X.Cases_03.04.2020"]
reg_data$marchCases_week1 = reg_data[, "X.Cases_03.04.2020"] - reg_data[, "X.Cases_02.26.2020"]
reg_data$febCases_week4 = reg_data[, "X.Cases_02.26.2020"] - reg_data[, "X.Cases_02.19.2020"]
reg_data$febCases_week3 = reg_data[, "X.Cases_02.19.2020"] - reg_data[, "X.Cases_02.12.2020"]
reg_data$febCases_week2 = reg_data[, "X.Cases_02.12.2020"] - reg_data[, "X.Cases_02.05.2020"]
reg_data$febCases_week1 = reg_data[, "X.Cases_02.05.2020"] - reg_data[, "X.Cases_01.29.2020"]
reg_data$janCases_week4 = reg_data[, "X.Cases_01.29.2020"] - reg_data[, "X.Cases_01.22.2020"]
reg_data = select(reg_data,-c(84:260))

# Removing variables which like serial numbers etc and total deaths
reg_data$X = NULL
reg_data$countyFIPS = NULL
reg_data$StateName = NULL
reg_data$CountyName = NULL
reg_data$Rural.UrbanContinuumCode2013 = as.factor(reg_data$Rural.UrbanContinuumCode2013)

# Foregin travel ban only has one level
reg_data$foreign.travel.ban = NULL

# Federal guideline has no variation
reg_data$federal.guidelines = NULL
reg_data$State = NULL
```

```{r test-train-split-reg, echo = FALSE, warning = FALSE, message = FALSE}
set.seed(42)
split_reg = initial_split(reg_data, prop = 0.80)
train_reg = training(split_reg)
test_reg = testing(split_reg)
```

```{r, cv-control-rf-reg, echo = FALSE}
cv_control = trainControl(method = "cv",
                          number = 5)
```

```{r, rf-reg, echo = FALSE, message = FALSE, cache = TRUE}
set.seed(42)
mod_rf_reg = train(
  form = deathscurrentweek ~ .,
  data = train_reg,
  method = "rf",
  trControl = cv_control,
  metric = 'RMSE',
  tuneLength = 10,
  verbose = FALSE,
  importance = TRUE
)
```


```{r penalized-reg, warning = FALSE, message = FALSE, echo = FALSE, cache = TRUE}
tuneGrid_lasso = expand.grid(.alpha = seq(0, 1, by = 0.5),
                             .lambda = seq(0, 100, by = 0.1))

trainControl <- trainControl(method = "cv",
                             number = 5)
set.seed(42)
mod_lasso = train(
  form = deathscurrentweek ~ .,
  data = train_reg,
  method = "glmnet",
  trControl = trainControl,
  metric = "RMSE",
  tuneGrid = tuneGrid_lasso,
  family = "gaussian"
)

```

```{r xgb-reg, echo = FALSE, cache = TRUE, message = FALSE, warning = FALSE}
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
  nrounds = 1000,
  eta = c(0.01, 0.001, 0.0001),
  max_depth = c(2, 4, 6, 8, 10),
  gamma = 1,
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = seq(0.5, 0.9, length.out = 5)
)

# pack the training control parameters
xgb_trcontrol_1 = trainControl(
  method = "cv",
  number = 5,
  verboseIter = FALSE,
  allowParallel = FALSE
)
set.seed(42)
xgb_train_1 = train(
  form = deathscurrentweek ~ .,
  data = train_reg,
  trControl = xgb_trcontrol_1,
  tuneGrid = xgb_grid_1,
  method = "xgbTree",
  metric = "RMSE"
)
```

```{r tune-result, echo = FALSE, warning = FALSE, message = FALSE}
cat("Tuned Parameters for Random Forest:")
mod_rf_reg$bestTune
cat("Tuned Parameters for Random Forest:")
mod_lasso$bestTune
cat("Tuned Parameters for XG Boost:")
xgb_train_1$bestTune
```


```{r reg-results, echo = FALSE, message = FALSE, warning = FALSE}
result = tibble(
  "Model Name" = c("Random Forest",
                   "XG Boost",
                   "Penalized Linear Regression") ,
  "RMSE" = c(
    min(mod_rf_reg$results[2]),
    min(xgb_train_1$results[8]),
    min(mod_lasso$results[3])
  )
)

kable(result, digits = 5,
      caption = "Cross Validated RMSE for Random Forest, XGBoost and Penalized LR") %>%
  kable_styling(
    "striped",
    full_width = FALSE,
    latex_options = c("striped", "hold_position")
  ) %>%
  column_spec(column = 1, bold = TRUE)

```

Looking at the RMSE calculated for the models using cross-validation, we can see that penalized Linear Regression has the lowest RMSE. Based on this we will use this model for our final predictions on the data of April 29. Before making predictions, we trained this model on the full data as earlier it was trained on sub-sample of the full data. This will allow the model to learn the patterns better and make better predictions. Below we have shown the RMSE calculated on the test data for the three models, Random Forest is working the best here but we cant use this knowledge to make our modelling decisions since in real settings we don't have the luxury of testing our models on test data which is why modelling decisions are based on RMSE decisions.

```{r test-data, warning = FALSE, message = FALSE, echo = FALSE, include= FALSE}
set.seed(42)
pred_test = predict(mod_lasso, test_reg)
test_rmse_lr = sqrt(mean((test_reg$deathscurrentweek - pred_test) ^ 2))
pred_test = predict(mod_rf_reg, test_reg)
test_rmse_rf = sqrt(mean((test_reg$deathscurrentweek - pred_test) ^ 2))
pred_test = predict(xgb_train_1, test_reg)
test_rmse_xgb = sqrt(mean((test_reg$deathscurrentweek - pred_test) ^ 2))
```

Variable Importance is analysed for the three models and top-5 most important variables for each of the model are shown below. For Linear models,  the absolute value of the t-statistic for each model parameter is used. For Random Forest, the MSE is computed on the out-of-bag data for each tree, and then the same computed after permuting a variable. The differences are averaged and normalized by the standard error. For boosting method, the variable importance uses the same approach as a single tree, but sums the importances over each boosting iteration. 


```{r varimpreg, echo = FALSE, warning = FALSE, message = FALSE, include = FALSE}
xgbimp = varImp(xgb_train_1, scale = FALSE)
xgbimp = as.data.frame(xgbimp$importance)
xgbimp = cbind(var_name = rownames(xgbimp), xgbimp)
rownames(xgbimp) = 1:nrow(xgbimp)
xgbimp = xgbimp[xgbimp$Overall > 0,]
xgbimp = xgbimp[order(xgbimp$Overall, decreasing = TRUE), ]

rf_imp <- varImp(mod_rf_reg, scale = FALSE)
rf_imp = as.data.frame(rf_imp$importance)
rf_imp = cbind(var_name = rownames(rf_imp), rf_imp)
rownames(rf_imp) = 1:nrow(rf_imp)
rf_imp = rf_imp[rf_imp$Overall > 0,]
rf_imp = rf_imp[order(rf_imp$Overall, decreasing = TRUE), ]

lr_imp <- varImp(mod_lasso, scale = FALSE)
lr_imp = as.data.frame(lr_imp$importance)
lr_imp <- cbind(var_name = rownames(lr_imp), lr_imp)
rownames(lr_imp) <- 1:nrow(lr_imp)
lr_imp = lr_imp[lr_imp$Overall > 0,]
lr_imp = lr_imp[order(lr_imp$Overall, decreasing = TRUE), ]
```

From the Variable Importance tables, we can see that the deaths in the previous few weeks has been the most effective in making predictions for the upcoming weeks. This means that we are able to capture the time series pattern well through our Machine Learning models and are able to effectively calculate the slope with the deaths are increasing. 

```{r varimpvisreg, echo = FALSE, warning = FALSE, message = FALSE}
kable(xgbimp[1:5, ],
      digits = 5,
      caption = "Variable Importance for prediction(XGB Model)",
      align = "clc") %>%
  kable_styling(
    c("striped", "bordered", "hover"),
    full_width = FALSE,
    latex_options = c("striped", "hold_position")
  )

kable(rf_imp[1:5, ],
      digits = 5,
      caption = "Variable Importance for prediction(RF Model)",
      align = "clc") %>%
  kable_styling(
    c("striped", "bordered", "hover"),
    full_width = FALSE,
    latex_options = c("striped", "hold_position")
  ) 

kable(lr_imp[1:5, ],
      digits = 5,
      caption = "Variable Importance for prediction(PLR Model)",
      align = "clc") %>%
  kable_styling(
    c("striped", "bordered", "hover"),
    full_width = FALSE,
    latex_options = c("striped", "hold_position")
  )
```

```{r test-rmse, warning=F, message=F, echo = FALSE}
result_rmse = tibble(
  "Model Name" = c("Penalized Linear Regression",
                   "Random Forest",
                   "XG Boost"
                   ) ,
  "Test RMSE" = c(
    test_rmse_lr,
    test_rmse_rf,
    test_rmse_xgb
  )
)

kable(result_rmse, digits = 5,
      caption = "Test RMSE for Random Forest, XGBoost and Penalized LR") %>%
  kable_styling(
    "striped",
    full_width = FALSE,
    latex_options = c("striped", "hold_position")
  ) %>%
  column_spec(column = 1, bold = TRUE)

```

```{r xgb-reg-full, echo = FALSE, cache = TRUE, message = FALSE, warning = FALSE}
# # set up the cross-validated hyper-parameter search
# xgb_grid_1 = expand.grid(
#   nrounds = xgb_train_1$bestTune$nrounds,
#   eta = xgb_train_1$bestTune$eta,
#   max_depth = xgb_train_1$bestTune$max_depth,
#   gamma = xgb_train_1$bestTune$gamma,
#   min_child_weight = xgb_train_1$bestTune$min_child_weight,
#   subsample = xgb_train_1$bestTune$subsample,
#   colsample_bytree = xgb_train_1$bestTune$colsample_bytree
# )
# 
# # pack the training control parameters
# xgb_trcontrol_1 = trainControl(
#   method = "cv",
#   number = 5,
#   verboseIter = FALSE,
#   allowParallel = TRUE
# )
# set.seed(42)
# xgb_train_full = train(
#   form = deathscurrentweek ~ .,
#   data = train_reg,
#   trControl = xgb_trcontrol_1,
#   tuneGrid = xgb_grid_1,
#   method = "xgbTree",
#   objective = "reg:squarederror",
#   metric = "RMSE"
# )
```

```{r, mod-lasso-full, echo = FALSE}
tuneGrid_lasso = expand.grid(.alpha = mod_lasso$bestTune$alpha,
                             .lambda = mod_lasso$bestTune$lambda)

trainControl <- trainControl(method = "cv",
                             number = 5)
set.seed(42)
mod_lasso_full = train(
  form = deathscurrentweek ~ .,
  data = reg_data,
  method = "glmnet",
  trControl = trainControl,
  metric = "RMSE",
  tuneGrid = tuneGrid_lasso,
  family = "gaussian"
)


```

### Making predictions for April 29

Once we have our models trained and tested, we decided to make predictions on the completely unseen and new data. This data is downloaded using python script present on the github repo mentioned above where we got the original data from. We will only keep the data till Apr 29 in this data-set and delete the columns containing data of the days after that. 

We will replicate the missing value imputation process that we applied on the original data to get rid of missing data in this data-set. Similar to the data-set above, we will employ feature engineering to create variables calculating weekly change in the deaths and cases count. Since we are predicting for one week ahead, and we have to keep the same features as the training data-set, the first week of the cases i.e Jan 22-29 gets dropped from the data. Once we have the data-set ready in the format same as the training data, we will make predictions using the Penalized Linear Regression method and compare it with the actuals and calculate the Root Mean Square Error to evaluate the performance.  

```{r actual-data, echo = FALSE, message = FALSE, warning = FALSE}
actual_apr29 = read.csv("Apr29.csv")
actual_deaths = actual_apr29$X.Deaths_04.29.2020[-2055]
actual_apr29 = select(actual_apr29,-c(290:313))
actual_apr29 = select(actual_apr29,-c(180:197))
```

```{r actual-data-missing-values, echo = FALSE, message = FALSE, warning = FALSE}
# Pop_latitude and Pop_latitude almost identical to lat and long and dont have missing values
actual_apr29$lat = NULL
actual_apr29$lon = NULL

# For missing values in mortality column, replacing NA values with national average seems reasonable
actual_apr29$X3.YrMortalityAge.1Year2015.17[is.na(actual_apr29$X3.YrMortalityAge.1Year2015.17)] = mean(actual_apr29$X3.YrMortalityAge.1Year2015.17, na.rm = TRUE)

# For 7 counties missing eligible for care, national eligiblity mean % was applied
national_covered_mean = mean(
  actual_apr29$X.EligibleforMedicare2018 / actual_apr29$PopulationEstimate2018,
  na.rm = TRUE
)
actual_apr29$X.EligibleforMedicare2018[is.na(actual_apr29$X.EligibleforMedicare2018)] = national_covered_mean * actual_apr29$PopulationEstimate2018[is.na(actual_apr29$X.EligibleforMedicare2018)]

# For Medicare Enrollement Aged - all misisng values replaced with zero
actual_apr29$MedicareEnrollment.AgedTot2017[is.na(actual_apr29$MedicareEnrollment.AgedTot2017)] = 0

actual_apr29$DiabetesPercentage[is.na(actual_apr29$DiabetesPercentage)] = mean(actual_apr29$DiabetesPercentage, na.rm = TRUE)
actual_apr29$HeartDiseaseMortality[is.na(actual_apr29$HeartDiseaseMortality)] = mean(actual_apr29$HeartDiseaseMortality, na.rm = TRUE)
actual_apr29$StrokeMortality[is.na(actual_apr29$StrokeMortality)] = mean(actual_apr29$StrokeMortality, na.rm = TRUE)
actual_apr29$dem_to_rep_ratio[is.na(actual_apr29$dem_to_rep_ratio)] = mean(actual_apr29$dem_to_rep_ratio, na.rm = TRUE)

# Foreign travel ban column only has one value which we think represent the ban being implemented hence we can convert into a factor variable

actual_apr29$foreign.travel.ban = 1
actual_apr29$foreign.travel.ban = as.factor(actual_apr29$foreign.travel.ban)

# Assuming that missing values in stay-at-home column represent the stay-at-home order was not issued at all
actual_apr29$stay.at.home[is.na(actual_apr29$stay.at.home)] = 999999
actual_apr29$X.50.gatherings[is.na(actual_apr29$X.50.gatherings)] = 999999
actual_apr29$X.500.gatherings[is.na(actual_apr29$X.500.gatherings)] = 999999
actual_apr29$entertainment.gym[is.na(actual_apr29$entertainment.gym)] = 999999
# Too many missing values for columns, no reasonable way to get any info from them
actual_apr29$X3.YrMortalityAge1.4Years2015.17 = NULL
actual_apr29$X3.YrMortalityAge5.14Years2015.17 = NULL
actual_apr29$mortality2015.17Estimated = NULL

actual_apr29$SVIPercentile[is.na(actual_apr29$SVIPercentile)] = median(actual_apr29$SVIPercentile, na.rm = TRUE)
actual_apr29$HPSAShortage[is.na(actual_apr29$HPSAShortage)] = mean(actual_apr29$HPSAShortage, na.rm = TRUE)

# All counties with 3 Yr diabetes missing are very small in population so assuming 0 for those
actual_apr29$X3.YrDiabetes2015.17[is.na(actual_apr29$X3.YrDiabetes2015.17)] = 0


# Mortality rates age wise ar eimputed using random forest method in the mice package for imputation
set.seed(42)
imputed = mice(
  actual_apr29[, 64:71],
  method = "rf",
  m = 3,
  maxit = 3,
  printFlag = FALSE
)
completed_data = complete(imputed, 1)
actual_apr29[, 64:71] = completed_data
imputed = mice(
  actual_apr29[, 13:83],
  method = "rf",
  m = 3,
  maxit = 3,
  printFlag = FALSE
)
completed_data = complete(imputed, 1)
actual_apr29[, 13:83] = completed_data

actual_apr29 = actual_apr29[-2055, ]

rm(completed_data, imputed)

```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
reg_data = actual_apr29
target = reg_data$X.Deaths_04.22.2020
reg_data$last7daysdeaths = reg_data[, "X.Deaths_04.22.2020"] - reg_data[, "X.Deaths_04.15.2020"]
reg_data$prev7daysdeaths = reg_data[, "X.Deaths_04.15.2020"] - reg_data[, "X.Deaths_04.08.2020"]
reg_data$marchweek5 = reg_data[, "X.Deaths_04.08.2020"] - reg_data[, "X.Deaths_04.01.2020"]
reg_data$marchweek4 = reg_data[, "X.Deaths_04.01.2020"] - reg_data[, "X.Deaths_03.25.2020"]
reg_data$marchweek3 = reg_data[, "X.Deaths_03.25.2020"] - reg_data[, "X.Deaths_03.18.2020"]
reg_data$marchweek2 = reg_data[, "X.Deaths_03.18.2020"] - reg_data[, "X.Deaths_03.11.2020"]
reg_data$marchweek1 = reg_data[, "X.Deaths_03.11.2020"] - reg_data[, "X.Deaths_03.04.2020"]
reg_data$febweek4 = reg_data[, "X.Deaths_03.04.2020"] - reg_data[, "X.Deaths_02.26.2020"]
reg_data$febweek3 = reg_data[, "X.Deaths_02.26.2020"] - reg_data[, "X.Deaths_02.19.2020"]
reg_data$febweek2 = reg_data[, "X.Deaths_02.19.2020"] - reg_data[, "X.Deaths_02.12.2020"]
reg_data$febweek1 = reg_data[, "X.Deaths_02.12.2020"] - reg_data[, "X.Deaths_02.05.2020"]
reg_data$janweek4 = reg_data[, "X.Deaths_02.05.2020"] - reg_data[, "X.Deaths_01.29.2020"]
reg_data$last7dayscases = reg_data[, "X.Cases_04.22.2020"] - reg_data[, "X.Cases_04.15.2020"]
reg_data$prev7dayscases = reg_data[, "X.Cases_04.15.2020"] - reg_data[, "X.Cases_04.08.2020"]
reg_data$marchCases_week5 = reg_data[, "X.Cases_04.08.2020"] - reg_data[, "X.Cases_04.01.2020"]
reg_data$marchCases_week4 = reg_data[, "X.Cases_04.01.2020"] - reg_data[, "X.Cases_03.25.2020"]
reg_data$marchCases_week3 = reg_data[, "X.Cases_03.25.2020"] - reg_data[, "X.Cases_03.18.2020"]
reg_data$marchCases_week2 = reg_data[, "X.Cases_03.18.2020"] - reg_data[, "X.Cases_03.11.2020"]
reg_data$marchCases_week1 = reg_data[, "X.Cases_03.11.2020"] - reg_data[, "X.Cases_03.04.2020"]
reg_data$febCases_week4 = reg_data[, "X.Cases_03.04.2020"] - reg_data[, "X.Cases_02.26.2020"]
reg_data$febCases_week3 = reg_data[, "X.Cases_02.26.2020"] - reg_data[, "X.Cases_02.19.2020"]
reg_data$febCases_week2 = reg_data[, "X.Cases_02.19.2020"] - reg_data[, "X.Cases_02.12.2020"]
reg_data$febCases_week1 = reg_data[, "X.Cases_02.12.2020"] - reg_data[, "X.Cases_02.05.2020"]
reg_data$janCases_week4 = reg_data[, "X.Cases_02.05.2020"] - reg_data[, "X.Cases_01.29.2020"]
reg_data = select(reg_data,-c(83:266))

# Removing variables which like serial numbers etc and total deaths
reg_data$X = NULL
reg_data$StateName = NULL
reg_data$countyFIPS = NULL
reg_data$CountyName = NULL
reg_data$Rural.UrbanContinuumCode2013 = as.factor(reg_data$Rural.UrbanContinuumCode2013)

# Foregin travel ban only has one level
reg_data$foreign.travel.ban = NULL

# Federal guideline has no variation
reg_data$federal.guidelines = NULL
reg_data$State = NULL

```

```{r future-pred, echo = FALSE, message = FALSE, warning = FALSE}

data_test = reg_data
set.seed(42)
pred_29april = predict(mod_lasso_full, data_test)
total_deaths_apr29 = pred_29april + target
pred_data = tibble("County_FIPS" = actual_apr29$countyFIPS,
                   "total_deaths_Apr29" = total_deaths_apr29)
pred_data$total_deaths_Apr29 = ceiling(pred_data$total_deaths_Apr29)
pred_data$lastweekdeaths = ceiling(pred_29april)
pred_data$deathsuptoApr22 = target
pred_data$Actual_deaths_Apr29 = actual_deaths
rmse_actual = sqrt(mean((pred_data$Actual_deaths_Apr29 - pred_data$total_deaths_Apr29) ^
                          2
))
plot(
  log(pred_data$Actual_deaths_Apr29),
  log(pred_data$total_deaths_Apr29),
  main = "Log of Actuals vs Predicted Deaths Till April 29",
  xlab = "Actual Deaths",
  ylab = "Predicted Deaths",
  col = "deepskyblue",
  sub = "Logarithmic Scale",
  pch = 19,
  cex = 1
)
abline(0, 1, col = "darkorange", lty = 1, lwd = 3)
```

The Root Mean Square Error of our predictions as compared to the actual deaths till April 29 is `r rmse_actual` which is pretty good in the context of the situation. In the graph above, we can see that model predictions are more inaccurate for the counties with very low death counts but is relatively much more accurate with counties with more deaths, this is a promising sign because if we can predict accurately the death count in more heavily hit counties, we can then focus on those and take necessary steps to combat the expected rise. 

There are many improvements we can consider to improve the performance, one we have already done is to train our final model on the full data till April 22 instead of a sub-sample before making predictions for the new data i.e data till April 29. As we see from the plot above, we notice that predictions become more inaccurate for counties where the death count is low, one improvement we can do remedy this is to split the counties into clusters based on the death count and train separate models for each cluster. Another positive aspect of the approach we took to prepare the data is that we drop the oldest week from the data as we get new data for the latest week, in time series often the most recent data points are more effective in predictions and the oldest points become less and less important, this is used well in the current settings

\newpage

# Collaborator's Questions

## Question: what population is the most vulnerable to this virus?

From our clustering analysis in the above sections, we can say that people living in counties from cluster 2 and 5 shown in Table 2 are the most affected. These are the counties in New York, California, Texas and Florida mainly. All these states are very densely populated are attractive tourist locations as travelers from the around the world visit these places. The international travel started the outbreak in the regions and the high population density led it to spread quickly and effectively. 

According to the Social Vulnerability Index, counties in Clusters 2 and 5 have a higher percentile than counties in Clusters 3 and 4, indicating that the population in these counties are more vulnerable than others based on social factors such as  poverty level, unemployment, income, high school diploma, housing, transportation amongst others.

We also examined top 50 counties with population above 55 and found that 48 of them belonged to cluster 2 and 5, supporting the fact that older population is at a higher mortality risk.

The counties in cluster 2 and 5 with highest number of deaths are performing worst in terms of ICU beds to total confirmed cases as shown in \autoref{mylabel2}.c, this is a very important factor highlighting that the healthcare facilities were not up to mark in these counties which resulted in more deaths.

## Question: what could we do to reduce mortality?

One of the most pressing questions of this crisis is damage control and reducing the number of cases and deaths. A way to go about this is to increase testing capacity across the country, as more the number of cases reported, the more data we will have to study the impact of this disease further and help in staying ahead of the curve.

Healthcare capacity remains to be an important factor contributing to the mortality, and this is elucidated across our analysis. Both unsupervised and supervised measure have shed the light on shortages of ICU beds, healthcare professionals as well as increased risk to those with heart, diabetes and stroke ailments. One way to tackle this problem is for states and counties to share their resources with highly affected regions. Based on our predictions, we can calculate the expected number of deaths and cases, and the counties which have a lower expectation of mortality and infection rate can provide help to counties which are still expected to be on the higher end of the spectrum. They can partner with hospitals to move medical staff support, ICU beds and medical instruments to areas which face an acute shortage of the them.

Another way to combat high mortality rates would be to recognize socially vulnerable population segments, based on age, gender, income, disability, health concerns and focus on helping these communities. These groups should be provided help in terms of daily needs in order to reduce their interaction with others. Local governments can mobilize less vulnerable population to volunteer for home delivery of medicines and groceries. Economically weaker sections should be helped to pay rents, buy necessities, utilities in order to prevent them going far out and about looking for work.   

\newpage 

# Appendix

```{r visuals-1,fig.cap="\\label{mylabel}Within groups Sum of Square vs Clusters.", fig.height = 4, fig.width = 6, echo = FALSE}
ggplot(as.data.frame(cbind(wss, seq(1,30))), aes(x = V2, y = wss)) +
  geom_line(linetype = "dashed") +
  geom_point() + 
  #ggtitle("Fig 1 - Within groups Sum of Square vs Clusters") +
  xlab("Number of Clusters") + ylab("Within groups sum of squares")
```

\newpage

```{r visuals-2,fig.cap="\\label{mylabel3}K-Means clustering of US Counties", fig.height = 6, fig.width = 10, echo = FALSE}
p02
```

\newpage

```{r visuals-4, fig.cap="\\label{mylabel2} K-Means Cluster Analysis",fig.height = 12, fig.width = 12, echo = FALSE}
grid.arrange(p03, p04, p05, p06, p07, p08, nrow = 3, ncol = 2)
```

```{r visuals-3,fig.cap="\\label{mylabel4} Hierarchical clustering of US Counties", fig.height = 6, fig.width = 10, echo = FALSE}
p09
```

```{r hcscree, fig.cap="\\label{mylabel5} Variance Explained with PCA dimensions for HC", echo = FALSE, fig.height = 6, fig.width = 6}
fviz_screeplot(pca_results, addlabels = TRUE, ylim = c(0, 75))

```

```{r visuals-5, fig.height = 8, fig.width = 12, echo = FALSE}
#p09
```

\newpage

```{r sommaps, fig.cap="\\label{mylabel6} Self-Organizing Map", fig.height = 8, fig.width = 12, echo = FALSE}

##Grid for SOM graphs
par(mfrow=c(1,2))
plot(map)
plot(map, type = "count")
par(oma=c(0,0,2,0))
title("Self-Organizing Map Plots" , outer=TRUE)
#mtext("SOM", cex = 1.5, outer = TRUE)
```

```{r somtab, fig.height = 8, fig.width = 12, echo = FALSE}
som_clusters = tibble(SOM_clusters)
kable(som_clusters, digits = 5,
      caption = "Clusters of SOM") %>%
  kable_styling(
    "striped",
    full_width = FALSE,
    latex_options = c("striped", "hold_position")
  ) %>%
  column_spec(column = 1, bold = TRUE)
```


[^1]: [COVID19 data + modeling at the county-level + hospital-level](https://github.com/Yu-Group/covid19-severity-prediction)
[^2]: [Curating a COVID-19 data repository and forecasting
county-level death counts in the United States](https://www.stat.berkeley.edu/~binyu/ps/papers2020/covid19_paper.pdf)
[^3]: [Estimation of the reproductive number of novel coronavirus (COVID-19) and the probable outbreak size on the Diamond Princess cruise ship: A data-driven analysis](https://www.sciencedirect.com/science/article/pii/S1201971220300916)
[^4]: [Prediction of the Epidemic Peak of Coronavirus Disease in Japan, 2020](https://www.mdpi.com/2077-0383/9/3/789#cite)

```{r, echo = FALSE}
knitr::knit_exit()
```
